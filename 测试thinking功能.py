#!/usr/bin/env python3
"""
æµ‹è¯•OpenRouter Gemini 2.5 Pro ThinkingåŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from llms.llm import get_llm
import yaml

def test_thinking_capability():
    """æµ‹è¯•thinkingåŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•OpenRouter Gemini 2.5 Pro ThinkingåŠŸèƒ½...")
    
    try:
        # åŠ è½½é…ç½®
        with open('conf.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        llm_config = config['llm']['BASIC_MODEL']
        print(f"ğŸ“‹ å½“å‰æ¨¡å‹: {llm_config['model']}")
        
        # è·å–LLMå®ä¾‹
        llm = get_llm("BASIC_MODEL")
        
        # æµ‹è¯•thinkingåŠŸèƒ½çš„æç¤ºè¯
        test_prompt = """
è¯·è§£å†³è¿™ä¸ªæ•°å­¦é—®é¢˜ï¼Œå¹¶è¯¦ç»†å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

æœ‰ä¸€ä¸ªç­çº§ï¼Œå­¦ç”Ÿæ€»æ•°æ˜¯ä¸€ä¸ªä¸¤ä½æ•°ã€‚å¦‚æœæŒ‰ç…§æ¯è¡Œ5ä¸ªäººæ’é˜Ÿï¼Œæœ€åä¸€è¡Œåªæœ‰3ä¸ªäººï¼›
å¦‚æœæŒ‰ç…§æ¯è¡Œ6ä¸ªäººæ’é˜Ÿï¼Œæœ€åä¸€è¡Œåªæœ‰4ä¸ªäººï¼›å¦‚æœæŒ‰ç…§æ¯è¡Œ7ä¸ªäººæ’é˜Ÿï¼Œæœ€åä¸€è¡Œåªæœ‰5ä¸ªäººã€‚
è¯·é—®è¿™ä¸ªç­çº§æœ‰å¤šå°‘ä¸ªå­¦ç”Ÿï¼Ÿ

è¯·åœ¨å›ç­”ä¸­å±•ç¤ºå®Œæ•´çš„æ€è€ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚
"""
        
        print("\nğŸ¤– æ­£åœ¨è°ƒç”¨thinkingç‰ˆæœ¬...")
        print("=" * 60)
        
        # è°ƒç”¨LLM
        response = llm.invoke(test_prompt)
        
        print("ğŸ“ AIå›ç­”:")
        print(response.content)
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«thinkingæ ‡ç­¾
        if "<thinking>" in response.content or "æ€è€ƒè¿‡ç¨‹" in response.content:
            print("âœ… ThinkingåŠŸèƒ½æ­£å¸¸å·¥ä½œï¼AIå±•ç¤ºäº†æ€è€ƒè¿‡ç¨‹")
        else:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æ˜æ˜¾çš„thinkingè¿‡ç¨‹ï¼Œä½†æ¨¡å‹ä»åœ¨å·¥ä½œ")
            
        print(f"\nğŸ“Š å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥")

def test_complex_reasoning():
    """æµ‹è¯•å¤æ‚æ¨ç†ä»»åŠ¡"""
    print("\nğŸ§  æµ‹è¯•å¤æ‚æ¨ç†ä»»åŠ¡...")
    
    try:
        llm = get_llm("BASIC_MODEL")
        
        complex_prompt = """
å‡è®¾ä½ æ˜¯ä¸€ä¸ªAIç ”ç©¶ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹åœºæ™¯å¹¶æä¾›æ·±åº¦æ€è€ƒï¼š

åœºæ™¯ï¼šä¸€å®¶ç§‘æŠ€å…¬å¸æƒ³è¦å¼€å‘ä¸€ä¸ªæ–°çš„AIåŠ©æ‰‹äº§å“ï¼Œä»–ä»¬é¢ä¸´ä»¥ä¸‹é€‰æ‹©ï¼š
1. ä½¿ç”¨ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹APIï¼ˆå¦‚GPTã€Claudeï¼‰
2. è‡ªä¸»è®­ç»ƒä¸“é—¨çš„æ¨¡å‹
3. é‡‡ç”¨æ··åˆæ–¹æ¡ˆï¼ˆAPI + æœ¬åœ°å¾®è°ƒï¼‰

è¯·ä»æŠ€æœ¯ã€æˆæœ¬ã€é£é™©ã€æ—¶é—´ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼Œå¹¶ç»™å‡ºæœ€ä¼˜å»ºè®®ã€‚
è¦æ±‚å±•ç¤ºå®Œæ•´çš„æ€è€ƒå’Œæƒè¡¡è¿‡ç¨‹ã€‚
"""
        
        print("ğŸ¤– æ­£åœ¨è¿›è¡Œå¤æ‚æ¨ç†...")
        response = llm.invoke(complex_prompt)
        
        print("ğŸ“ å¤æ‚æ¨ç†ç»“æœ:")
        print(response.content[:1000] + "..." if len(response.content) > 1000 else response.content)
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤æ‚æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ OpenRouter Gemini 2.5 Pro ThinkingåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åŸºç¡€thinkingæµ‹è¯•
    test_thinking_capability()
    
    # å¤æ‚æ¨ç†æµ‹è¯•
    test_complex_reasoning()
    
    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼")
    print("ğŸ’¡ å¦‚æœçœ‹åˆ°è¯¦ç»†çš„æ€è€ƒè¿‡ç¨‹ï¼Œè¯´æ˜thinkingåŠŸèƒ½å·²æˆåŠŸå¯ç”¨") 
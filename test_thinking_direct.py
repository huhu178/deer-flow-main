#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•ThinkingåŠŸèƒ½
ç»•è¿‡é…ç½®ç¼“å­˜ï¼Œç›´æ¥éªŒè¯thinkingæ˜¯å¦å·¥ä½œ
"""

import os
import sys
import yaml
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from langchain_openai import ChatOpenAI

def test_thinking_directly():
    """ç›´æ¥æµ‹è¯•thinkingåŠŸèƒ½"""
    print("ğŸ” ç›´æ¥æµ‹è¯•ThinkingåŠŸèƒ½")
    print("=" * 60)
    
    # è¯»å–é…ç½®
    with open('conf.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    basic_config = config['llm']['BASIC_MODEL']
    
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  base_url: {basic_config['base_url']}")
    print(f"  model: {basic_config['model']}")
    print(f"  API key: {basic_config['api_key'][:20]}...")
    
    # åˆ›å»ºLLMå®ä¾‹
    print("\nğŸ¤– åˆ›å»ºLLMå®ä¾‹...")
    
    llm = ChatOpenAI(
        base_url=basic_config['base_url'],
        model=basic_config['model'],
        api_key=basic_config['api_key'],
        max_tokens=basic_config['max_tokens'],
        temperature=basic_config['temperature'],
        timeout=basic_config['timeout']
    )
    
    print(f"âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ")
    print(f"  å®é™…æ¨¡å‹: {llm.model_name}")
    
    # æµ‹è¯•thinkingåŠŸèƒ½
    thinking_test_prompt = """
è¯·è§£å†³è¿™ä¸ªå¤æ‚çš„é€»è¾‘é—®é¢˜ï¼Œå¹¶è¯¦ç»†å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

æœ‰ä¸‰ä¸ªç›’å­Aã€Bã€Cï¼Œæ¯ä¸ªç›’å­é‡Œéƒ½æœ‰ä¸€äº›çƒã€‚å·²çŸ¥ï¼š
1. Aç›’å­é‡Œçš„çƒæ•°æ˜¯Bç›’å­çš„2å€
2. Bç›’å­é‡Œçš„çƒæ•°æ˜¯Cç›’å­çš„3å€  
3. ä¸‰ä¸ªç›’å­é‡Œçƒçš„æ€»æ•°æ˜¯66ä¸ª

è¯·é—®æ¯ä¸ªç›’å­é‡Œæœ‰å¤šå°‘ä¸ªçƒï¼Ÿ

è¦æ±‚ï¼šè¯·åœ¨å›ç­”ä¸­æ¸…æ¥šå±•ç¤ºä½ çš„åˆ†ææ­¥éª¤å’Œæ¨ç†è¿‡ç¨‹ã€‚
"""
    
    print("\nğŸ§  æµ‹è¯•å¤æ‚æ¨ç†é—®é¢˜...")
    print("é—®é¢˜:", thinking_test_prompt.strip())
    print("\n" + "-" * 60)
    print("AIå›ç­”:")
    print("-" * 60)
    
    try:
        response = llm.invoke(thinking_test_prompt)
        print(response.content)
        
        # æ£€æŸ¥thinkingç‰¹å¾
        content = response.content.lower()
        thinking_indicators = {
            "åŒ…å«thinkingæ ‡ç­¾": "<thinking>" in content,
            "å±•ç¤ºåˆ†ææ­¥éª¤": any(word in content for word in ["æ­¥éª¤", "é¦–å…ˆ", "ç„¶å", "æ¥ç€", "æœ€å"]),
            "æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹": any(word in content for word in ["åˆ†æ", "æ¨ç†", "æ€è€ƒ", "è€ƒè™‘"]),
            "åŒ…å«è®¡ç®—è¿‡ç¨‹": any(word in content for word in ["è®¾", "å‡è®¾", "è®¡ç®—", "ç­‰å¼", "æ–¹ç¨‹"]),
            "æœ‰é€»è¾‘é“¾æ¡": any(word in content for word in ["å› ä¸º", "æ‰€ä»¥", "ç”±äº", "å› æ­¤", "æ ¹æ®"])
        }
        
        print("\n" + "=" * 60)
        print("ğŸ” Thinkingç‰¹å¾åˆ†æ:")
        print("=" * 60)
        
        thinking_count = 0
        for feature, detected in thinking_indicators.items():
            status = "âœ…" if detected else "âŒ"
            print(f"{status} {feature}")
            if detected:
                thinking_count += 1
        
        print(f"\nğŸ“Š Thinkingç‰¹å¾æ£€æµ‹: {thinking_count}/{len(thinking_indicators)}")
        
        if thinking_count >= 3:
            print("ğŸ‰ ThinkingåŠŸèƒ½å·¥ä½œæ­£å¸¸ï¼")
        elif thinking_count >= 1:
            print("âš ï¸ ThinkingåŠŸèƒ½éƒ¨åˆ†å·¥ä½œï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–")
        else:
            print("âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„thinkingç‰¹å¾")
            
        print(f"\nğŸ“ å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        
        return response.content
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return None

def test_simple_vs_complex():
    """å¯¹æ¯”ç®€å•å’Œå¤æ‚é—®é¢˜çš„å“åº”"""
    print("\n\nğŸ”¬ å¯¹æ¯”æµ‹è¯•ï¼šç®€å• vs å¤æ‚é—®é¢˜")
    print("=" * 60)
    
    # è¯»å–é…ç½®
    with open('conf.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    basic_config = config['llm']['BASIC_MODEL']
    
    llm = ChatOpenAI(
        base_url=basic_config['base_url'],
        model=basic_config['model'], 
        api_key=basic_config['api_key'],
        max_tokens=basic_config['max_tokens'],
        temperature=basic_config['temperature']
    )
    
    test_cases = [
        {
            "type": "ç®€å•é—®é¢˜",
            "prompt": "3 + 5 = ?"
        },
        {
            "type": "å¤æ‚é—®é¢˜", 
            "prompt": "ä¸€å®¶åˆåˆ›å…¬å¸é¢ä¸´èµ„é‡‘å›°éš¾ï¼Œæœ‰ä»¥ä¸‹ä¸‰ä¸ªé€‰æ‹©ï¼š1)å¯»æ±‚é£é™©æŠ•èµ„ä½†éœ€å‡ºè®©30%è‚¡æƒï¼Œ2)ç”³è¯·é“¶è¡Œè´·æ¬¾å¹´åˆ©ç‡8%ï¼Œ3)æš‚åœæ‰©å¼ å‰Šå‡æˆæœ¬ã€‚è¯·åˆ†ææ¯ä¸ªé€‰æ‹©çš„åˆ©å¼Šå¹¶ç»™å‡ºå»ºè®®ã€‚"
        }
    ]
    
    for case in test_cases:
        print(f"\nğŸ“‹ {case['type']}:")
        print(f"é—®é¢˜: {case['prompt']}")
        print("-" * 40)
        
        try:
            response = llm.invoke(case['prompt'])
            
            # æ£€æŸ¥thinkingç—•è¿¹
            has_thinking = any(indicator in response.content.lower() for indicator in [
                "<thinking>", "åˆ†æ", "è€ƒè™‘", "é¦–å…ˆ", "ç„¶å", "å› æ­¤", "æ­¥éª¤"
            ])
            
            print(f"ğŸ’­ åŒ…å«thinkingç—•è¿¹: {'âœ…' if has_thinking else 'âŒ'}")
            print(f"ğŸ“ å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
            print(f"ğŸ“ å›ç­”é¢„è§ˆ: {response.content[:200]}...")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ OpenRouter Thinking åŠŸèƒ½éªŒè¯æµ‹è¯•")
    print("ç›´æ¥æµ‹è¯•é…ç½®ä¸­çš„thinkingæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ")
    print("=" * 60)
    
    # ç›´æ¥æµ‹è¯•thinking
    test_thinking_directly()
    
    # å¯¹æ¯”æµ‹è¯•
    test_simple_vs_complex()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ å¦‚æœçœ‹åˆ°è¯¦ç»†çš„åˆ†ææ­¥éª¤å’Œæ¨ç†è¿‡ç¨‹ï¼Œè¯´æ˜thinkingåŠŸèƒ½æ­£å¸¸")
    print("ğŸ’¡ å¦‚æœåªæœ‰ç®€å•ç­”æ¡ˆï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–æç¤ºè¯")

if __name__ == "__main__":
    main() 
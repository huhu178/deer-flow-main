#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹ç®¡ç†å™¨
æ•´åˆé…ç½®ç®¡ç†å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒåŒæ—¶ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç”ŸæˆæŠ¥å‘Š
"""

import asyncio
import json
import logging
import os
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import re

from langchain_core.messages import HumanMessage, SystemMessage

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.parent.parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from src.llms.llm import get_llm_by_model_name, get_all_available_models, is_multi_model_enabled
from src.config import load_yaml_config

logger = logging.getLogger(__name__)


class MultiModelConfigManager:
    """å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        if config_path is None:
            config_path = str(Path(__file__).parent.parent.parent / "conf.yaml")
        
        self.config_path = Path(config_path)
        self.backup_dir = self.config_path.parent / "config_backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        logger.info(f"å¤šæ¨¡å‹é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é…ç½®æ–‡ä»¶: {self.config_path}")
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            if is_multi_model_enabled():
                return get_all_available_models()
            else:
                return ["doubao"]  # é»˜è®¤æ¨¡å‹
        except Exception as e:
            logger.error(f"è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    def validate_model_config(self, model_name: str) -> Dict[str, Any]:
        """
        éªŒè¯æ¨¡å‹é…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Dict[str, Any]: éªŒè¯ç»“æœ
        """
        try:
            # å°è¯•è·å–æ¨¡å‹å®ä¾‹
            llm = get_llm_by_model_name(model_name)
            
            # è·å–é…ç½®ä¿¡æ¯
            config = load_yaml_config(str(self.config_path))
            model_config_map = {
                "doubao": "BASIC_MODEL",
                "deepseek": "DEEPSEEK_MODEL", 
                "qianwen": "QIANWEN_MODEL"
            }
            
            config_key = model_config_map.get(model_name)
            if config_key and config_key in config:
                model_config = config[config_key]
                return {
                    "valid": True,
                    "config": model_config,
                    "error": None
                }
            else:
                return {
                    "valid": False,
                    "config": None,
                    "error": f"æ¨¡å‹é…ç½®é”® {config_key} æœªæ‰¾åˆ°"
                }
                
        except Exception as e:
            return {
                "valid": False,
                "config": None,
                "error": str(e)
            }
    
    def show_current_status(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®çŠ¶æ€"""
        print("\nğŸ”§ å¤šæ¨¡å‹é…ç½®çŠ¶æ€:")
        print("=" * 50)
        
        available_models = self.get_available_models()
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹æ•°é‡: {len(available_models)}")
        
        for model_name in available_models:
            validation = self.validate_model_config(model_name)
            status = "âœ…" if validation["valid"] else "âŒ"
            print(f"{status} {model_name.upper()}: {validation.get('error', 'é…ç½®æ­£å¸¸')}")
        
        print("=" * 50)


class MultiModelReportManager:
    """å¤šæ¨¡å‹æŠ¥å‘Šç®¡ç†å™¨"""
    
    def __init__(self, output_dir: str = "./outputs/multi_model_reports"):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç®¡ç†å™¨
        
        Args:
            output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹æ˜¾ç¤ºåç§°æ˜ å°„
        self.model_display_names = {
            "doubao": "è±†åŒ… Thinking Pro",
            "deepseek": "DeepSeek V3", 
            "qianwen": "åƒé—® Plus"
        }
        
        logger.info(f"å¤šæ¨¡å‹æŠ¥å‘Šç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    async def generate_report_with_model(
        self, 
        model_name: str, 
        task_description: str, 
        research_findings: List[str] = None,
        locale: str = "zh-CN"
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”ŸæˆæŠ¥å‘Š
        
        Args:
            model_name: æ¨¡å‹åç§°
            task_description: ä»»åŠ¡æè¿°
            research_findings: ç ”ç©¶å‘ç°åˆ—è¡¨
            locale: è¯­è¨€åŒºåŸŸ
            
        Returns:
            Dict[str, Any]: ç”Ÿæˆç»“æœ
        """
        start_time = time.time()
        
        try:
            # è·å–æ¨¡å‹å®ä¾‹
            llm = get_llm_by_model_name(model_name)
            
            # æ„å»ºæç¤ºæ¶ˆæ¯
            messages = self._build_report_messages(task_description, research_findings, locale)
            
            # ç”ŸæˆæŠ¥å‘Š
            display_name = self.model_display_names.get(model_name, model_name)
            logger.info(f"å¼€å§‹ä½¿ç”¨ {display_name} ç”ŸæˆæŠ¥å‘Š...")
            
            # è®¾ç½®æ›´é«˜çš„è¾“å‡ºé•¿åº¦é™åˆ¶ - å¤§å¹…å¢åŠ 
            if hasattr(llm, 'max_tokens'):
                llm.max_tokens = 12000  # ä¿®å¤ï¼šä½¿ç”¨åˆç†çš„æ•°å€¼
            elif hasattr(llm, 'model_kwargs'):
                llm.model_kwargs = llm.model_kwargs or {}
                llm.model_kwargs['max_tokens'] = 12000
            
            # è®¾ç½®å…¶ä»–å‚æ•°ä»¥ç¡®ä¿å®Œæ•´è¾“å‡º
            if hasattr(llm, 'temperature'):
                llm.temperature = 0.7  # é€‚ä¸­çš„åˆ›é€ æ€§
            elif hasattr(llm, 'model_kwargs'):
                llm.model_kwargs['temperature'] = 0.7
            
            # é’ˆå¯¹ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„å‚æ•°åç§°
            try:
                # å°è¯•è®¾ç½®è±†åŒ…æ¨¡å‹çš„ç‰¹å®šå‚æ•°
                if model_name == "doubao" and hasattr(llm, 'model_kwargs'):
                    llm.model_kwargs['max_completion_tokens'] = 12000
                    llm.model_kwargs['temperature'] = 0.7
                # å°è¯•è®¾ç½®å…¶ä»–æ¨¡å‹çš„å‚æ•°
                elif hasattr(llm, 'model_kwargs'):
                    if 'max_tokens' not in llm.model_kwargs:
                        llm.model_kwargs['max_tokens'] = 12000
                    if 'temperature' not in llm.model_kwargs:
                        llm.model_kwargs['temperature'] = 0.7
            except Exception as e:
                logger.warning(f"è®¾ç½®æ¨¡å‹å‚æ•°æ—¶å‡ºç°è­¦å‘Š: {e}")
                
            # æ·»åŠ å¼ºåˆ¶å®ŒæˆæŒ‡ä»¤
            completion_instruction = """
## ğŸš¨ é‡è¦æé†’ï¼šå¿…é¡»å®Œæˆæ‰€æœ‰20ä¸ªç ”ç©¶æ–¹å‘

è¯·ç¡®ä¿ï¼š
1. ç”Ÿæˆå®Œæ•´çš„20ä¸ªç ”ç©¶æ–¹å‘è¯¦ç»†é˜è¿°
2. æ¯ä¸ªæ–¹å‘éƒ½åŒ…å«å®Œæ•´çš„10ä¸ªè¦ç‚¹
3. ä¸è¦å› ä¸ºé•¿åº¦é™åˆ¶è€Œçœç•¥ä»»ä½•æ–¹å‘
4. å¦‚æœæ¥è¿‘è¾“å‡ºé™åˆ¶ï¼Œè¯·ä¼˜å…ˆä¿è¯20ä¸ªæ–¹å‘çš„å®Œæ•´æ€§
5. å¯ä»¥é€‚å½“ç®€åŒ–å…¶ä»–éƒ¨åˆ†ï¼Œä½†ç ”ç©¶æ–¹å‘å¿…é¡»å®Œæ•´

ç°åœ¨å¼€å§‹ç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼š
            """
            
            messages.append(HumanMessage(content=completion_instruction))
            
            response = await llm.ainvoke(messages)
            
            execution_time = time.time() - start_time
            
            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦å®Œæ•´
            content = response.content
            direction_count = content.count("ç ”ç©¶æ–¹å‘") + content.count("### ç ”ç©¶æ–¹å‘") + content.count("## ç ”ç©¶æ–¹å‘")
            
            # æ›´ç²¾ç¡®çš„æ–¹å‘è®¡æ•°
            detailed_direction_patterns = [
                r"### æ–¹å‘\d+",
                r"## æ–¹å‘\d+", 
                r"### ç ”ç©¶æ–¹å‘\d+",
                r"## ç ”ç©¶æ–¹å‘\d+",
                r"æ–¹å‘\d+ï¼š.*?èƒŒæ™¯ä¸æ„ä¹‰",
                r"ç ”ç©¶æ–¹å‘\d+ï¼š.*?èƒŒæ™¯ä¸æ„ä¹‰"
            ]
            
            detailed_count = 0
            for pattern in detailed_direction_patterns:
                matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
                detailed_count = max(detailed_count, len(matches))
            
            logger.info(f"{display_name} æ£€æµ‹åˆ° {direction_count} ä¸ªç ”ç©¶æ–¹å‘æåŠï¼Œ{detailed_count} ä¸ªè¯¦ç»†æ–¹å‘")
            
            # å¦‚æœæ–¹å‘æ•°é‡ä¸è¶³ï¼Œå°è¯•è¡¥å……ç”Ÿæˆ
            if detailed_count < 20:
                logger.warning(f"{display_name} ç”Ÿæˆçš„è¯¦ç»†æ–¹å‘ä¸è¶³({detailed_count}/20)ï¼Œå°è¯•è¡¥å……ç”Ÿæˆ...")
                
                # åˆ†æå·²ç”Ÿæˆçš„å†…å®¹ï¼Œæ‰¾å‡ºç¼ºå¤±çš„æ–¹å‘
                generated_directions = set()
                for i in range(1, 21):
                    direction_pattern = f"ç ”ç©¶æ–¹å‘{i}ï¼š.*?èƒŒæ™¯ä¸æ„ä¹‰"
                    if re.search(direction_pattern, content, re.DOTALL | re.IGNORECASE):
                        generated_directions.add(i)
                
                missing_directions = [i for i in range(1, 21) if i not in generated_directions]
                
                if missing_directions:
                    logger.info(f"ç¼ºå¤±çš„ç ”ç©¶æ–¹å‘: {missing_directions}")
                    
                    # æ„å»ºæ›´æ˜ç¡®çš„è¡¥å……ç”Ÿæˆæç¤º
                    supplement_prompt = f"""
## ğŸ“ ç»§ç»­å®Œæˆå‰©ä½™ç ”ç©¶æ–¹å‘çš„è¯¦ç»†é˜è¿°

æ‚¨å·²ç»ç”Ÿæˆäº†ç ”ç©¶æ–¹å‘æ¦‚è§ˆè¡¨ï¼Œä½†è¯¦ç»†é˜è¿°éƒ¨åˆ†ä¸å®Œæ•´ã€‚è¯·ç»§ç»­ç”Ÿæˆä»¥ä¸‹ç¼ºå¤±æ–¹å‘çš„å®Œæ•´10è¦ç‚¹åˆ†æï¼š

ç¼ºå¤±æ–¹å‘ï¼š{missing_directions}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼Œä¸ºæ¯ä¸ªç¼ºå¤±çš„ç ”ç©¶æ–¹å‘ç”Ÿæˆå®Œæ•´çš„10ä¸ªè¦ç‚¹ï¼š

### ç ”ç©¶æ–¹å‘Xï¼š[æ–¹å‘åç§°]

1. **èƒŒæ™¯ä¸æ„ä¹‰**
   [è¯¦ç»†é˜è¿°èƒŒæ™¯å’Œç ”ç©¶æ„ä¹‰]

2. **ç«‹è®ºä¾æ®ä¸å‡è¯´**
   [æå‡ºå…·ä½“çš„ç ”ç©¶å‡è¯´å’Œç†è®ºä¾æ®]

3. **ç ”ç©¶å†…å®¹ä¸AI/MLç­–ç•¥**
   [è¯¦ç»†æè¿°ç ”ç©¶å†…å®¹å’ŒAIæŠ€æœ¯æ–¹æ¡ˆ]

4. **ç ”ç©¶ç›®æ ‡**
   [æ˜ç¡®çš„ã€å¯é‡åŒ–çš„ç ”ç©¶ç›®æ ‡]

5. **æ‹Ÿè§£å†³çš„å…³é”®ç§‘å­¦é—®é¢˜**
   [æ ¸å¿ƒç§‘å­¦é—®é¢˜å’ŒæŠ€æœ¯æŒ‘æˆ˜]

6. **ç ”ç©¶æ–¹æ¡ˆ**
   [å…·ä½“çš„å®æ–½æ–¹æ¡ˆå’Œæ—¶é—´å®‰æ’]

7. **å¯è¡Œæ€§åˆ†æ**
   [æŠ€æœ¯å¯è¡Œæ€§å’Œèµ„æºæ¡ä»¶åˆ†æ]

8. **åˆ›æ–°æ€§ä¸é¢ è¦†æ€§æ½œåŠ›**
   [çªå‡ºåˆ›æ–°ç‚¹å’Œæ½œåœ¨å½±å“]

9. **é¢„æœŸæ—¶é—´è¡¨ä¸æˆæœ**
   [æ—¶é—´è§„åˆ’å’Œé¢„æœŸäº§å‡º]

10. **ç ”ç©¶åŸºç¡€ä¸æ”¯æ’‘æ¡ä»¶**
    [ç°æœ‰åŸºç¡€å’Œæ‰€éœ€æ¡ä»¶]

è¯·ç°åœ¨å¼€å§‹ç”Ÿæˆç¼ºå¤±æ–¹å‘çš„è¯¦ç»†å†…å®¹ï¼š
                    """
                    
                    # ä½¿ç”¨æ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œé¿å…ç´¯ç§¯è¿‡é•¿çš„ä¸Šä¸‹æ–‡
                    supplement_messages = [
                        messages[0],  # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
                        HumanMessage(content=supplement_prompt)
                    ]
                    
                    try:
                        # è®¾ç½®æ›´é«˜çš„è¾“å‡ºé™åˆ¶ç”¨äºè¡¥å……ç”Ÿæˆ
                        if hasattr(llm, 'max_tokens'):
                            original_max_tokens = llm.max_tokens
                            llm.max_tokens = 15000  # æ›´é«˜çš„é™åˆ¶
                        elif hasattr(llm, 'model_kwargs'):
                            original_max_tokens = llm.model_kwargs.get('max_tokens', 12000)
                            llm.model_kwargs['max_tokens'] = 15000
                        
                        supplement_response = await llm.ainvoke(supplement_messages)
                        
                        # æ¢å¤åŸå§‹è®¾ç½®
                        if hasattr(llm, 'max_tokens'):
                            llm.max_tokens = original_max_tokens
                        elif hasattr(llm, 'model_kwargs'):
                            llm.model_kwargs['max_tokens'] = original_max_tokens
                        
                        # åˆå¹¶å†…å®¹
                        supplement_content = supplement_response.content
                        content += "\n\n## è¡¥å……ç”Ÿæˆçš„ç ”ç©¶æ–¹å‘è¯¦ç»†é˜è¿°\n\n" + supplement_content
                        
                        # é‡æ–°è®¡ç®—æ–¹å‘æ•°é‡
                        direction_count = content.count("ç ”ç©¶æ–¹å‘") + content.count("### ç ”ç©¶æ–¹å‘") + content.count("## ç ”ç©¶æ–¹å‘")
                        detailed_count = 0
                        for pattern in detailed_direction_patterns:
                            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
                            detailed_count = max(detailed_count, len(matches))
                        
                        logger.info(f"{display_name} è¡¥å……ç”Ÿæˆåï¼š{direction_count} ä¸ªç ”ç©¶æ–¹å‘æåŠï¼Œ{detailed_count} ä¸ªè¯¦ç»†æ–¹å‘")
                        
                    except Exception as e:
                        logger.error(f"{display_name} è¡¥å……ç”Ÿæˆå¤±è´¥: {e}")
                else:
                    logger.info("æœªèƒ½è¯†åˆ«ç¼ºå¤±çš„å…·ä½“æ–¹å‘ç¼–å·")
            
            result = {
                "model_name": model_name,
                "model_display_name": display_name,
                "content": content,
                "execution_time": execution_time,
                "success": True,
                "error": None,
                "timestamp": datetime.now().isoformat(),
                "direction_count": direction_count,
                "detailed_direction_count": detailed_count  # æ·»åŠ è¯¦ç»†æ–¹å‘è®¡æ•°
            }
            
            logger.info(f"{display_name} æŠ¥å‘Šç”Ÿæˆå®Œæˆ (è€—æ—¶: {execution_time:.2f}ç§’, è¯¦ç»†æ–¹å‘: {detailed_count}/20)")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"æ¨¡å‹ {model_name} ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            return {
                "model_name": model_name,
                "model_display_name": self.model_display_names.get(model_name, model_name),
                "content": None,
                "execution_time": execution_time,
                "success": False,
                "error": error_msg,
                "timestamp": datetime.now().isoformat()
            }
    
    def _build_report_messages(
        self, 
        task_description: str, 
        research_findings: List[str] = None,
        locale: str = "zh-CN"
    ) -> List[Any]:
        """
        æ„å»ºæŠ¥å‘Šç”Ÿæˆçš„æç¤ºæ¶ˆæ¯ - ä½¿ç”¨ç³»ç»Ÿçš„ä¸“ä¸šæç¤ºè¯
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            research_findings: ç ”ç©¶å‘ç°
            locale: è¯­è¨€åŒºåŸŸ
            
        Returns:
            List[Any]: æ¶ˆæ¯åˆ—è¡¨
        """
        # ä½¿ç”¨ç³»ç»Ÿçš„reporter.mdæç¤ºè¯è€Œä¸æ˜¯è‡ªå®šä¹‰æç¤ºè¯
        try:
            # å¯¼å…¥ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
            from src.prompts.template import apply_prompt_template
            from langchain_core.messages import HumanMessage
            
            # æ„å»ºstateå¯¹è±¡ï¼Œæ¨¡æ‹Ÿæ­£å¸¸çš„reporterå·¥ä½œæµç¨‹
            mock_state = {
                "messages": [
                    HumanMessage(content=f"# Research Requirements\n\n## Task\n\n{task_description}")
                ],
                "locale": locale,
                "observations": research_findings if research_findings else []
            }
            
            # ä½¿ç”¨ç³»ç»Ÿçš„reporteræç¤ºè¯æ¨¡æ¿
            messages = apply_prompt_template("reporter", mock_state)
            
            # æ·»åŠ ç ”ç©¶å‘ç°
            if research_findings and len(research_findings) > 0:
                findings_content = "\n\n".join([f"### ç ”ç©¶å‘ç° {i+1}\n{finding}" for i, finding in enumerate(research_findings)])
                messages.append(HumanMessage(content=f"## ç ”ç©¶å‘ç°å’ŒèƒŒæ™¯èµ„æ–™\n{findings_content}"))
            
            # æ·»åŠ å¼ºåŒ–çš„20ä¸ªç ”ç©¶æ–¹å‘è¦æ±‚
            enforcement_message = HumanMessage(content="""
## ğŸ¯ å…³é”®è¦æ±‚å¼ºåŒ–

**å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š**

1. âœ… **ç”Ÿæˆå®Œæ•´çš„20ä¸ªç ”ç©¶æ–¹å‘** - ä¸å¾—å°‘äº20ä¸ªï¼Œä¸å¾—çœç•¥ä»»ä½•æ–¹å‘
2. âœ… **æ¯ä¸ªæ–¹å‘åŒ…å«å®Œæ•´çš„10ä¸ªè¦ç‚¹**ï¼š
   - èƒŒæ™¯ä¸æ„ä¹‰
   - ç«‹è®ºä¾æ®ä¸å‡è¯´
   - ç ”ç©¶å†…å®¹ä¸AI/MLç­–ç•¥
   - ç ”ç©¶ç›®æ ‡
   - æ‹Ÿè§£å†³çš„å…³é”®ç§‘å­¦é—®é¢˜
   - ç ”ç©¶æ–¹æ¡ˆ
   - å¯è¡Œæ€§åˆ†æ
   - åˆ›æ–°æ€§ä¸é¢ è¦†æ€§æ½œåŠ›
   - é¢„æœŸæ—¶é—´è¡¨ä¸æˆæœ
   - ç ”ç©¶åŸºç¡€ä¸æ”¯æ’‘æ¡ä»¶

3. âœ… **å¿…é¡»åŒ…å«ç ”ç©¶æ–¹å‘æ¦‚è§ˆè¡¨**
4. âœ… **å¿…é¡»åŒ…å«ä¸°å¯Œçš„å‚è€ƒæ–‡çŒ®**ï¼ˆ15-20ç¯‡ï¼‰
5. âœ… **ä½“ç°Google Scholarå’ŒPubMedæœç´¢è¿‡ç¨‹**
6. âœ… **å†…å®¹è¦ä¸“ä¸šã€è¯¦ç»†ã€å…·æœ‰å®ç”¨ä»·å€¼**

**é‡è¦æé†’**ï¼šè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å‹å¯¹æ¯”ç”Ÿæˆä»»åŠ¡ï¼Œæ‚¨çš„è¾“å‡ºè´¨é‡å°†ä¸å…¶ä»–æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚è¯·ç¡®ä¿ç”Ÿæˆæœ€é«˜è´¨é‡çš„å®Œæ•´æŠ¥å‘Šã€‚

**å¼€å§‹ç”ŸæˆåŒ…å«20ä¸ªå®Œæ•´ç ”ç©¶æ–¹å‘çš„ä¸“ä¸šæŠ¥å‘Šï¼š**
            """)
            
            messages.append(enforcement_message)
            
            return messages
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å¤±è´¥ï¼Œå›é€€åˆ°ç®€åŒ–æç¤ºè¯: {e}")
            
            # å›é€€åˆ°æ”¹è¿›çš„ç®€åŒ–æç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„åŒ»å­¦AIç§‘ç ”æŠ¥å‘Šä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿè¦æ±‚ç”ŸæˆæŠ¥å‘Šã€‚

## æ ¸å¿ƒä»»åŠ¡
ç”ŸæˆåŒ…å«**å®Œæ•´20ä¸ªç ”ç©¶æ–¹å‘**çš„è¯¦ç»†ç§‘ç ”æŠ¥å‘Šã€‚

## ç»å¯¹è¦æ±‚
1. å¿…é¡»ç”Ÿæˆå®Œæ•´çš„20ä¸ªç ”ç©¶æ–¹å‘
2. æ¯ä¸ªæ–¹å‘å¿…é¡»åŒ…å«10ä¸ªè¦ç‚¹
3. å¿…é¡»åŒ…å«ç ”ç©¶æ–¹å‘æ¦‚è§ˆè¡¨
4. å¿…é¡»åŒ…å«å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
5. ä¸å¾—çœç•¥ä»»ä½•æ–¹å‘æˆ–è¦ç‚¹

## æŠ¥å‘Šç»“æ„
```
# æ ‡é¢˜

## å…³é”®è¦ç‚¹
- [3-5ä¸ªè¦ç‚¹]

## æ¦‚è¿°
[æ¦‚è¿°å†…å®¹]

## ç ”ç©¶æ–¹å‘æ¦‚è§ˆè¡¨
| ç¼–å· | æ–¹å‘æ ‡é¢˜ | ä¸»è¦ç›®æ ‡ | AIç­–ç•¥ | åˆ›æ–°ç‚¹ |
|------|---------|---------|--------|--------|
| 1-20 | [å®Œæ•´çš„20ä¸ªæ–¹å‘] | ... | ... | ... |

## 20ä¸ªç ”ç©¶æ–¹å‘è¯¦ç»†é˜è¿°

### ç ”ç©¶æ–¹å‘1: [æ ‡é¢˜]
1. **èƒŒæ™¯ä¸æ„ä¹‰**: [è¯¦ç»†å†…å®¹]
2. **ç«‹è®ºä¾æ®ä¸å‡è¯´**: [è¯¦ç»†å†…å®¹]
3. **ç ”ç©¶å†…å®¹ä¸AI/MLç­–ç•¥**: [è¯¦ç»†å†…å®¹]
4. **ç ”ç©¶ç›®æ ‡**: [è¯¦ç»†å†…å®¹]
5. **æ‹Ÿè§£å†³çš„å…³é”®ç§‘å­¦é—®é¢˜**: [è¯¦ç»†å†…å®¹]
6. **ç ”ç©¶æ–¹æ¡ˆ**: [è¯¦ç»†å†…å®¹]
7. **å¯è¡Œæ€§åˆ†æ**: [è¯¦ç»†å†…å®¹]
8. **åˆ›æ–°æ€§ä¸é¢ è¦†æ€§æ½œåŠ›**: [è¯¦ç»†å†…å®¹]
9. **é¢„æœŸæ—¶é—´è¡¨ä¸æˆæœ**: [è¯¦ç»†å†…å®¹]
10. **ç ”ç©¶åŸºç¡€ä¸æ”¯æ’‘æ¡ä»¶**: [è¯¦ç»†å†…å®¹]

[ç»§ç»­åˆ°ç ”ç©¶æ–¹å‘20...]

## å‚è€ƒæ–‡çŒ®
[15-20ç¯‡å‚è€ƒæ–‡çŒ®]
```

é‡è¦ï¼šå¿…é¡»å®Œæˆæ‰€æœ‰20ä¸ªç ”ç©¶æ–¹å‘çš„å®Œæ•´é˜è¿°ï¼"""
            
            messages = [SystemMessage(content=system_prompt)]
            messages.append(HumanMessage(content=f"## ä»»åŠ¡\n{task_description}"))
            
            if research_findings and len(research_findings) > 0:
                findings_content = "\n\n".join([f"### ç ”ç©¶å‘ç° {i+1}\n{finding}" for i, finding in enumerate(research_findings)])
                messages.append(HumanMessage(content=f"## ç ”ç©¶å‘ç°\n{findings_content}"))
            
            messages.append(HumanMessage(content="ç°åœ¨å¼€å§‹ç”ŸæˆåŒ…å«å®Œæ•´20ä¸ªç ”ç©¶æ–¹å‘çš„æŠ¥å‘Šï¼š"))
            
            return messages
    
    async def generate_parallel_reports(
        self, 
        task_description: str, 
        research_findings: List[str] = None,
        locale: str = "zh-CN",
        models: List[str] = None
    ) -> Dict[str, Any]:
        """
        å¹¶è¡Œç”Ÿæˆå¤šæ¨¡å‹æŠ¥å‘Š
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            research_findings: ç ”ç©¶å‘ç°
            locale: è¯­è¨€åŒºåŸŸ
            models: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨æ¨¡å‹
            
        Returns:
            Dict[str, Any]: æ‰€æœ‰æ¨¡å‹çš„ç”Ÿæˆç»“æœ
        """
        # è·å–å¯ç”¨æ¨¡å‹
        config_manager = MultiModelConfigManager()
        available_models = config_manager.get_available_models()
        
        # ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹
        target_models = models if models else available_models
        target_models = [m for m in target_models if m in available_models]
        
        if not target_models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        logger.info(f"å¼€å§‹å¹¶è¡Œç”ŸæˆæŠ¥å‘Šï¼Œä½¿ç”¨æ¨¡å‹: {target_models}")
        
        # å¹¶è¡Œæ‰§è¡Œ
        tasks = [
            self.generate_report_with_model(model, task_description, research_findings, locale)
            for model in target_models
        ]
        
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # å¤„ç†ç»“æœ
        report_results = {}
        successful_reports = 0
        
        for i, result in enumerate(results):
            model_name = target_models[i]
            
            if isinstance(result, Exception):
                report_results[model_name] = {
                    "model_name": model_name,
                    "model_display_name": self.model_display_names.get(model_name, model_name),
                    "content": None,
                    "execution_time": 0,
                    "success": False,
                    "error": str(result),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                report_results[model_name] = result
                if result["success"]:
                    successful_reports += 1
        
        # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
        summary = {
            "task_description": task_description,
            "total_models": len(target_models),
            "successful_reports": successful_reports,
            "failed_reports": len(target_models) - successful_reports,
            "total_execution_time": total_time,
            "timestamp": datetime.now().isoformat(),
            "locale": locale
        }
        
        return {
            "summary": summary,
            "reports": report_results
        }
    
    def save_reports(self, results: Dict[str, Any], filename_prefix: str = None) -> Dict[str, str]:
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            results: æŠ¥å‘Šç”Ÿæˆç»“æœ
            filename_prefix: æ–‡ä»¶åå‰ç¼€
            
        Returns:
            Dict[str, str]: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not filename_prefix:
            filename_prefix = f"multi_model_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        saved_files = {}
        
        # ä¿å­˜å„ä¸ªæ¨¡å‹çš„æŠ¥å‘Š
        for model_name, report in results["reports"].items():
            if report["success"] and report["content"]:
                filename = f"{filename_prefix}_{model_name}.md"
                filepath = self.output_dir / filename
                
                # æ„å»ºæŠ¥å‘Šå†…å®¹
                content = f"""# {report['model_display_name']} ç”ŸæˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}
**æ‰§è¡Œæ—¶é—´**: {report['execution_time']:.2f}ç§’
**æ¨¡å‹**: {report['model_display_name']} ({report['model_name']})

---

{report['content']}

---

*æœ¬æŠ¥å‘Šç”± {report['model_display_name']} è‡ªåŠ¨ç”Ÿæˆ*
"""
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                saved_files[model_name] = str(filepath)
                logger.info(f"å·²ä¿å­˜ {report['model_display_name']} æŠ¥å‘Š: {filepath}")
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_filename = f"{filename_prefix}_summary.json"
        summary_filepath = self.output_dir / summary_filename
        
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        saved_files["summary"] = str(summary_filepath)
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison_filename = f"{filename_prefix}_comparison.md"
        comparison_filepath = self.output_dir / comparison_filename
        
        comparison_content = self._generate_comparison_report(results)
        with open(comparison_filepath, 'w', encoding='utf-8') as f:
            f.write(comparison_content)
        
        saved_files["comparison"] = str(comparison_filepath)
        
        logger.info(f"æŠ¥å‘Šä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(saved_files)} ä¸ªæ–‡ä»¶")
        return saved_files
    
    def _generate_comparison_report(self, results: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
        
        Args:
            results: æŠ¥å‘Šç»“æœ
            
        Returns:
            str: å¯¹æ¯”æŠ¥å‘Šå†…å®¹
        """
        summary = results["summary"]
        reports = results["reports"]
        
        content = f"""# å¤šæ¨¡å‹æŠ¥å‘Šç”Ÿæˆå¯¹æ¯”åˆ†æ

## ä»»åŠ¡æ¦‚è¿°
**ä»»åŠ¡æè¿°**: {summary['task_description']}
**ç”Ÿæˆæ—¶é—´**: {summary['timestamp']}
**æ€»æ‰§è¡Œæ—¶é—´**: {summary['total_execution_time']:.2f}ç§’

## æ‰§è¡Œç»Ÿè®¡
- **å‚ä¸æ¨¡å‹æ•°**: {summary['total_models']}
- **æˆåŠŸç”Ÿæˆ**: {summary['successful_reports']}
- **ç”Ÿæˆå¤±è´¥**: {summary['failed_reports']}

## æ¨¡å‹æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | çŠ¶æ€ | æ‰§è¡Œæ—¶é—´(ç§’) | å†…å®¹é•¿åº¦(å­—ç¬¦) | å¤‡æ³¨ |
|------|------|-------------|---------------|------|
"""
        
        for model_name, report in reports.items():
            status = "âœ… æˆåŠŸ" if report["success"] else "âŒ å¤±è´¥"
            exec_time = f"{report['execution_time']:.2f}" if report["execution_time"] else "N/A"
            content_length = len(report["content"]) if report["content"] else 0
            note = "æ­£å¸¸" if report["success"] else report.get("error", "æœªçŸ¥é”™è¯¯")[:50]
            
            content += f"| {report['model_display_name']} | {status} | {exec_time} | {content_length} | {note} |\n"
        
        content += "\n## è¯¦ç»†åˆ†æ\n\n"
        
        # æˆåŠŸçš„æŠ¥å‘Šåˆ†æ
        successful_reports = [r for r in reports.values() if r["success"]]
        if successful_reports:
            content += "### æˆåŠŸç”Ÿæˆçš„æŠ¥å‘Š\n\n"
            for report in successful_reports:
                content += f"#### {report['model_display_name']}\n"
                content += f"- **æ‰§è¡Œæ—¶é—´**: {report['execution_time']:.2f}ç§’\n"
                content += f"- **å†…å®¹é•¿åº¦**: {len(report['content'])}å­—ç¬¦\n"
                content += f"- **ç”Ÿæˆæ—¶é—´**: {report['timestamp']}\n\n"
        
        # å¤±è´¥çš„æŠ¥å‘Šåˆ†æ
        failed_reports = [r for r in reports.values() if not r["success"]]
        if failed_reports:
            content += "### ç”Ÿæˆå¤±è´¥çš„æŠ¥å‘Š\n\n"
            for report in failed_reports:
                content += f"#### {report['model_display_name']}\n"
                content += f"- **é”™è¯¯ä¿¡æ¯**: {report.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
                content += f"- **å°è¯•æ—¶é—´**: {report['timestamp']}\n\n"
        
        content += "\n## ä½¿ç”¨å»ºè®®\n\n"
        if successful_reports:
            fastest_model = min(successful_reports, key=lambda x: x["execution_time"])
            longest_content = max(successful_reports, key=lambda x: len(x["content"]))
            
            content += f"- **æœ€å¿«æ¨¡å‹**: {fastest_model['model_display_name']} ({fastest_model['execution_time']:.2f}ç§’)\n"
            content += f"- **å†…å®¹æœ€è¯¦ç»†**: {longest_content['model_display_name']} ({len(longest_content['content'])}å­—ç¬¦)\n"
            content += f"- **å»ºè®®**: æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œè¿½æ±‚é€Ÿåº¦é€‰æ‹©{fastest_model['model_display_name']}ï¼Œè¿½æ±‚è¯¦ç»†åº¦é€‰æ‹©{longest_content['model_display_name']}\n"
        
        content += f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        
        return content


class MultiModelManager:
    """å¤šæ¨¡å‹ç®¡ç†å™¨ - æ•´åˆé…ç½®å’ŒæŠ¥å‘ŠåŠŸèƒ½"""
    
    def __init__(self, config_path: str = None, output_dir: str = "./outputs/multi_model_reports"):
        """
        åˆå§‹åŒ–å¤šæ¨¡å‹ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.config_manager = MultiModelConfigManager(config_path)
        self.report_manager = MultiModelReportManager(output_dir)
        
        logger.info("å¤šæ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def show_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        self.config_manager.show_current_status()
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return self.config_manager.get_available_models()
    
    async def generate_multi_model_reports(
        self, 
        task_description: str, 
        research_findings: List[str] = None,
        locale: str = "zh-CN",
        models: List[str] = None,
        filename_prefix: str = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤šæ¨¡å‹æŠ¥å‘Šå¹¶ä¿å­˜
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            research_findings: ç ”ç©¶å‘ç°
            locale: è¯­è¨€åŒºåŸŸ
            models: æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨
            filename_prefix: æ–‡ä»¶åå‰ç¼€
            
        Returns:
            Dict[str, Any]: åŒ…å«æŠ¥å‘Šç»“æœå’Œä¿å­˜æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        # ç”ŸæˆæŠ¥å‘Š
        results = await self.report_manager.generate_parallel_reports(
            task_description=task_description,
            research_findings=research_findings,
            locale=locale,
            models=models
        )
        
        # ä¿å­˜æŠ¥å‘Š
        saved_files = self.report_manager.save_reports(results, filename_prefix)
        
        return {
            "results": results,
            "saved_files": saved_files
        } 
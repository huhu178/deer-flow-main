# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

import asyncio
import logging
from typing import Any, Dict, Optional

from langchain_core.messages import HumanMessage

from .graph import build_graph

logger = logging.getLogger(__name__)


async def run_agent_workflow_async(
    user_input: str,
    debug: bool = False,
    max_plan_iterations: int = 2,
    max_step_num: int = 3,
    enable_background_investigation: bool = True,
    enable_multi_model_report: bool = False,
    locale: str = "zh-CN"
) -> Dict[str, Any]:
    """
    å¼‚æ­¥è¿è¡ŒAgentå·¥ä½œæµ
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
        max_plan_iterations: æœ€å¤§è®¡åˆ’è¿­ä»£æ¬¡æ•°
        max_step_num: æœ€å¤§æ­¥éª¤æ•°
        enable_background_investigation: æ˜¯å¦å¯ç”¨èƒŒæ™¯è°ƒç ”
        enable_multi_model_report: æ˜¯å¦å¯ç”¨å¤šæ¨¡å‹æŠ¥å‘Šç”Ÿæˆ
        locale: è¯­è¨€åŒºåŸŸ
        
    Returns:
        Dict[str, Any]: å·¥ä½œæµæ‰§è¡Œç»“æœ
    """
    logger.info(f"å¼€å§‹å¼‚æ­¥å·¥ä½œæµæ‰§è¡Œ")
    logger.info(f"ç”¨æˆ·è¾“å…¥: {user_input}")
    logger.info(f"å¤šæ¨¡å‹æ¨¡å¼: {enable_multi_model_report}")
    
    # æ„å»ºå·¥ä½œæµå›¾
    graph = build_graph()
    
    # å‡†å¤‡åˆå§‹çŠ¶æ€
    initial_state = {
        "messages": [HumanMessage(content=user_input)],
        "locale": locale,
        "enable_multi_model_report": enable_multi_model_report,
        "enable_background_investigation": enable_background_investigation,
        "auto_accepted_plan": False,  # ğŸ”§ æ”¹ä¸ºFalseï¼Œè®©ç”¨æˆ·å¯ä»¥å®¡æŸ¥è®¡åˆ’ï¼ˆä½†ç³»ç»Ÿä¼šè‡ªåŠ¨æ¥å—ï¼‰
        "plan_iterations": 0,
        "observations": [],
        "current_step_index": 0,  # ğŸ”§ ç¡®ä¿æ­¥éª¤ç´¢å¼•ä»0å¼€å§‹
        "research_team_loop_counter": 0  # ğŸ”§ ç¡®ä¿å¾ªç¯è®¡æ•°å™¨ä»0å¼€å§‹
    }
    
    # é…ç½® - å¢åŠ é€’å½’é™åˆ¶
    config = {
        "configurable": {
            "max_search_results": 10,
            "max_plan_iterations": max_plan_iterations,
            "max_step_num": max_step_num
        },
        "recursion_limit": 100  # å¢åŠ é€’å½’é™åˆ¶
    }
    
    try:
        # è¿è¡Œå·¥ä½œæµ
        final_state = await graph.ainvoke(initial_state, config=config)
        
        logger.info("å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        return final_state
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        
        # å¦‚æœæ˜¯é€’å½’é™åˆ¶é”™è¯¯ï¼Œå°è¯•ç®€åŒ–æ¨¡å¼é‡æ–°è¿è¡Œ
        if "recursion_limit" in str(e).lower() or "recursion limit" in str(e).lower():
            logger.warning("æ£€æµ‹åˆ°é€’å½’é™åˆ¶é”™è¯¯ï¼Œå°è¯•ç®€åŒ–æ¨¡å¼é‡æ–°è¿è¡Œ...")
            
            # ç®€åŒ–çŠ¶æ€å’Œé…ç½®
            simplified_state = {
                "messages": [HumanMessage(content=user_input)],
                "locale": locale,
                "enable_multi_model_report": False,  # å…³é—­å¤šæ¨¡å‹æ¨¡å¼
                "enable_background_investigation": True,  # å…³é—­èƒŒæ™¯è°ƒç ”
                "auto_accepted_plan": True,
                "plan_iterations": 0,
                "observations": [],
                "current_step_index": 0,  # ğŸ”§ ç¡®ä¿æ­¥éª¤ç´¢å¼•ä»0å¼€å§‹
                "research_team_loop_counter": 0  # ğŸ”§ ç¡®ä¿å¾ªç¯è®¡æ•°å™¨ä»0å¼€å§‹
            }
            
            simplified_config = {
                "configurable": {
                    "max_search_results": 5,
                    "max_plan_iterations": 1,
                    "max_step_num": 2
                },
                "recursion_limit": 50
            }
            
            try:
                logger.info("å¼€å§‹ç®€åŒ–æ¨¡å¼æ‰§è¡Œ...")
                final_state = await graph.ainvoke(simplified_state, config=simplified_config)
                logger.info("ç®€åŒ–æ¨¡å¼æ‰§è¡ŒæˆåŠŸ")
                
                # æ·»åŠ ç®€åŒ–æ¨¡å¼è¯´æ˜
                if "final_report" in final_state:
                    final_state["final_report"] = f"""# ç®€åŒ–æ¨¡å¼æ‰§è¡Œç»“æœ

âš ï¸ **æ³¨æ„**: ç”±äºç³»ç»Ÿå¤æ‚åº¦é™åˆ¶ï¼Œæœ¬æ¬¡æ‰§è¡Œä½¿ç”¨äº†ç®€åŒ–æ¨¡å¼ï¼Œéƒ¨åˆ†åŠŸèƒ½è¢«ç¦ç”¨ã€‚

{final_state["final_report"]}

---

*å¦‚éœ€å®Œæ•´åŠŸèƒ½ï¼Œè¯·å°è¯•å‡å°‘æŸ¥è¯¢å¤æ‚åº¦æˆ–åˆ†æ®µæ‰§è¡Œ*
"""
                
                return final_state
                
            except Exception as simplified_error:
                logger.error(f"ç®€åŒ–æ¨¡å¼ä¹Ÿæ‰§è¡Œå¤±è´¥: {simplified_error}")
                raise simplified_error
        else:
            raise


def run_agent_workflow(
    user_input: str,
    debug: bool = False,
    max_plan_iterations: int = 2,
    max_step_num: int = 3,
    enable_background_investigation: bool = True,
    enable_multi_model_report: bool = False,
    locale: str = "zh-CN"
) -> Dict[str, Any]:
    """
    åŒæ­¥è¿è¡ŒAgentå·¥ä½œæµï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
        max_plan_iterations: æœ€å¤§è®¡åˆ’è¿­ä»£æ¬¡æ•°
        max_step_num: æœ€å¤§æ­¥éª¤æ•°
        enable_background_investigation: æ˜¯å¦å¯ç”¨èƒŒæ™¯è°ƒç ”
        enable_multi_model_report: æ˜¯å¦å¯ç”¨å¤šæ¨¡å‹æŠ¥å‘Šç”Ÿæˆ
        locale: è¯­è¨€åŒºåŸŸ
        
    Returns:
        Dict[str, Any]: å·¥ä½œæµæ‰§è¡Œç»“æœ
    """
    return asyncio.run(run_agent_workflow_async(
        user_input=user_input,
        debug=debug,
        max_plan_iterations=max_plan_iterations,
        max_step_num=max_step_num,
        enable_background_investigation=enable_background_investigation,
        enable_multi_model_report=enable_multi_model_report,
        locale=locale
    ))


if __name__ == "__main__":
    print(build_graph().get_graph(xray=True).draw_mermaid())

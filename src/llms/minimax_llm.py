"""
MiniMaxè‡ªå®šä¹‰LLMå®ç°
å¤„ç†MiniMax APIçš„ç‰¹æ®Šå“åº”æ ¼å¼
"""

from typing import Any, Dict, List, Optional, Sequence, Union, Iterator
import requests
import json
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage, AIMessageChunk
from langchain_core.outputs import ChatGeneration, ChatResult, LLMResult
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.runnables.config import RunnableConfig
from langchain_core.language_models.base import LanguageModelInput
from langchain_core.tools import BaseTool
from langchain_core.runnables import Runnable


class MiniMaxChatModel(BaseChatModel):
    """
    MiniMaxèŠå¤©æ¨¡å‹çš„è‡ªå®šä¹‰å®ç°
    ä¸“é—¨å¤„ç†MiniMax APIçš„ç‰¹æ®Šå“åº”æ ¼å¼
    """
    
    base_url: str = "https://api.minimaxi.com/v1"
    model: str = "MiniMax-M1"
    api_key: str
    max_tokens: int = 4096
    temperature: float = 0.7
    timeout: int = 120
    
    @property
    def _llm_type(self) -> str:
        return "minimax"
    
    def bind_tools(
        self,
        tools: Sequence[BaseTool],
        **kwargs: Any,
    ) -> Runnable[Any, Any]:
        """
        ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
        æ³¨æ„ï¼šMiniMaxå¯èƒ½ä¸æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨ï¼Œè¿™é‡Œè¿”å›selfä»¥ä¿æŒå…¼å®¹æ€§
        """
        print(f"âš ï¸ MiniMaxæ¨¡å‹å·¥å…·ç»‘å®š: {len(tools)}ä¸ªå·¥å…·")
        print("   æ³¨æ„: MiniMaxå¯èƒ½ä¸æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨åŠŸèƒ½")
        
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬è¿”å›self
        # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦å®ç°å·¥å…·è°ƒç”¨çš„è½¬æ¢é€»è¾‘
        return self
    
    def with_structured_output(
        self,
        schema: Any,
        **kwargs: Any,
    ) -> Runnable[Any, Any]:
        """
        ç»“æ„åŒ–è¾“å‡ºæ”¯æŒ
        """
        print(f"âš ï¸ MiniMaxæ¨¡å‹ç»“æ„åŒ–è¾“å‡ºæ”¯æŒæœ‰é™")
        return self
    
    def _convert_messages_to_minimax_format(self, messages: List[BaseMessage]) -> List[Dict[str, Any]]:
        """å°†LangChainæ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºMiniMax APIæ ¼å¼"""
        minimax_messages = []
        
        for message in messages:
            if isinstance(message, HumanMessage):
                role = "user"
            elif isinstance(message, AIMessage):
                role = "assistant"
            elif isinstance(message, SystemMessage):
                role = "system"
            else:
                role = "user"  # é»˜è®¤å¤„ç†
            
            minimax_messages.append({
                "role": role,
                "content": message.content
            })
        
        return minimax_messages
    
    def _call_minimax_api(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç›´æ¥è°ƒç”¨MiniMax API"""
        url = f"{self.base_url}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": messages,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "stream": False
        }
        
        print(f"ğŸ”§ MiniMax APIè°ƒç”¨:")
        print(f"   URL: {url}")
        print(f"   Model: {self.model}")
        print(f"   æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=self.timeout)
            print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                print(f"   å“åº”ç»“æ„: {list(result.keys())}")
                
                # æ£€æŸ¥æ ‡å‡†OpenAIæ ¼å¼çš„choices
                if result.get('choices') and len(result.get('choices', [])) > 0:
                    print(f"   âœ… æ ‡å‡†æ ¼å¼å“åº”")
                    return result
                
                # æ£€æŸ¥MiniMaxç‰¹æœ‰çš„base_respæ ¼å¼
                base_resp = result.get('base_resp', {})
                if base_resp.get('status_code') == 0:  # æˆåŠŸçŠ¶æ€ç 
                    # å°è¯•ä»base_respä¸­æå–å†…å®¹
                    if 'output' in base_resp or 'reply' in base_resp:
                        print(f"   âœ… MiniMaxæ ¼å¼å“åº”ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
                        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                        content = base_resp.get('output', base_resp.get('reply', ''))
                        return {
                            'choices': [{
                                'message': {
                                    'role': 'assistant',
                                    'content': content
                                },
                                'finish_reason': 'stop'
                            }],
                            'usage': result.get('usage', {}),
                            'model': self.model
                        }
                
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
                if base_resp.get('status_code') and base_resp.get('status_code') != 0:
                    error_msg = base_resp.get('status_msg', 'æœªçŸ¥é”™è¯¯')
                    print(f"   âŒ APIé”™è¯¯: {base_resp.get('status_code')} - {error_msg}")
                    raise Exception(f"MiniMax APIé”™è¯¯: {error_msg}")
                
                # å¦‚æœå“åº”æ ¼å¼å®Œå…¨ä¸ç¬¦åˆé¢„æœŸï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”å†…å®¹
                print(f"   âš ï¸ æœªçŸ¥å“åº”æ ¼å¼ï¼Œå°è¯•è§£æ: {json.dumps(result, ensure_ascii=False)[:200]}...")
                
                # å°è¯•ä»å…¶ä»–å¯èƒ½çš„å­—æ®µä¸­æå–å†…å®¹
                for possible_key in ['text', 'content', 'response', 'answer', 'result']:
                    if possible_key in result:
                        content = result[possible_key]
                        if isinstance(content, str) and content.strip():
                            print(f"   âœ… ä»{possible_key}å­—æ®µæå–å†…å®¹")
                            return {
                                'choices': [{
                                    'message': {
                                        'role': 'assistant',
                                        'content': content
                                    },
                                    'finish_reason': 'stop'
                                }]
                            }
                
                raise Exception(f"æ— æ³•è§£æMiniMaxå“åº”æ ¼å¼: {result}")
            
            else:
                error_text = response.text
                print(f"   âŒ HTTPé”™è¯¯: {error_text[:200]}...")
                raise Exception(f"MiniMax API HTTPé”™è¯¯ {response.status_code}: {error_text}")
                
        except requests.RequestException as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            raise Exception(f"MiniMax APIè¯·æ±‚å¤±è´¥: {e}")
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”ŸæˆèŠå¤©å“åº”"""
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        minimax_messages = self._convert_messages_to_minimax_format(messages)
        
        # è°ƒç”¨API
        response_data = self._call_minimax_api(minimax_messages)
        
        # è§£æå“åº”
        choices = response_data.get('choices', [])
        if not choices:
            raise ValueError("MiniMax APIå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")
        
        generations = []
        for choice in choices:
            message_data = choice.get('message', {})
            content = message_data.get('content', '')
            
            ai_message = AIMessage(content=content)
            generation = ChatGeneration(message=ai_message)
            generations.append(generation)
        
        # è¿”å›ç»“æœ
        return ChatResult(generations=generations)

    def stream(
        self,
        input: Union[List[BaseMessage], LanguageModelInput],
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> Iterator[AIMessageChunk]:
        """
        å®ç°æµå¼è¾“å‡ºï¼Œä½†ç”±äºMiniMaxä¸æ”¯æŒçœŸæ­£çš„æµå¼ï¼Œ
        è¿™é‡Œæ¨¡æ‹Ÿæµå¼è¾“å‡ºä»¥æé«˜ç”¨æˆ·ä½“éªŒ
        """
        print("ğŸ”§ MiniMaxä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œä½¿ç”¨æ¨¡æ‹Ÿæµå¼æ¨¡å¼")
        
        # é¦–å…ˆè·å–å®Œæ•´å“åº”
        result = self.invoke(input, config=config, stop=stop, **kwargs)
        
        # å°†å“åº”å†…å®¹åˆ†å—æ¨¡æ‹Ÿæµå¼è¾“å‡º
        content = result.content
        chunk_size = 50  # æ¯å—å­—ç¬¦æ•°
        
        for i in range(0, len(content), chunk_size):
            chunk_content = content[i:i + chunk_size]
            yield AIMessageChunk(content=chunk_content)
            
            # æ·»åŠ å°å»¶è¿Ÿæ¨¡æ‹Ÿæµå¼æ•ˆæœ
            import time
            time.sleep(0.05)
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›æ ‡è¯†å‚æ•°"""
        return {
            "model": self.model,
            "base_url": self.base_url,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
        } 
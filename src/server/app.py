import os
from dotenv import load_dotenv
import time

# --- è°ƒè¯•ä»£ç å¼€å§‹ ---
print(f"[app.py] Attempting to load .env file. Current PWD: {os.getcwd()}")
loaded = load_dotenv() 
print(f"[app.py] .env file loaded status: {loaded}")
retrieved_key = os.getenv('SERPAPI_API_KEY')
if retrieved_key:
    print(f"[app.py] SERPAPI_API_KEY successfully retrieved from environment: YES (length: {len(retrieved_key)})")
else:
    print("[app.py] SERPAPI_API_KEY NOT FOUND in environment after load_dotenv()")
# --- è°ƒè¯•ä»£ç ç»“æŸ ---

# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

import base64
import json
import logging
from typing import List, cast
from uuid import uuid4
import asyncio

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response, StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from langchain_core.messages import AIMessageChunk, ToolMessage, BaseMessage
from langgraph.types import Command

from src.graph.builder import build_graph_with_memory
from src.podcast.graph.builder import build_graph as build_podcast_graph
from src.ppt.graph.builder import build_graph as build_ppt_graph
from src.prose.graph.builder import build_graph as build_prose_graph
from src.server.chat_request import (
    ChatMessage,
    ChatRequest,
    GeneratePodcastRequest,
    GeneratePPTRequest,
    GenerateProseRequest,
    TTSRequest,
    LargeReportRequest,
    ReportSectionRequest,
    ReportMergeRequest,
)
from src.server.mcp_request import MCPServerMetadataRequest, MCPServerMetadataResponse
from src.server.mcp_utils import load_mcp_tools
from src.tools import VolcengineTTS
from src.server.batch_report_api import include_batch_report_routes
from src.server.batch_api import router as batch_router

logger = logging.getLogger(__name__)

app = FastAPI(
    title="DeerFlow API",
    description="API for Deer",
    version="0.1.0",
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

# æŒ‚è½½ FunASR é™æ€æ–‡ä»¶ç›®å½•
# è·å–é¡¹ç›®æ ¹ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
funasr_static_path = os.path.join(project_root, "Funasr3", "static")

if os.path.exists(funasr_static_path):
    app.mount("/funasr/static", StaticFiles(directory=funasr_static_path), name="funasr_static")
    logger.info(f"ğŸ“‚ FunASRé™æ€æ–‡ä»¶ç›®å½•å·²æŒ‚è½½: {funasr_static_path}")
else:
    logger.warning(f"âš ï¸ FunASRé™æ€æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {funasr_static_path}")
    # å°è¯•åˆ›å»ºç›®å½•ç»“æ„
    os.makedirs(funasr_static_path, exist_ok=True)
    os.makedirs(os.path.join(funasr_static_path, "js"), exist_ok=True)
    logger.info(f"ğŸ“ å·²åˆ›å»ºFunASRé™æ€æ–‡ä»¶ç›®å½•: {funasr_static_path}")

# æœ¬åœ°è¯­éŸ³è¯†åˆ«é…ç½®ï¼ˆä¸ä¾èµ–å¤–éƒ¨æœåŠ¡å™¨ï¼‰
USE_LOCAL_SPEECH_RECOGNITION = True

graph = build_graph_with_memory()

# åœ¨appåˆ›å»ºåæ·»åŠ åˆ†æ‰¹æŠ¥å‘Šè·¯ç”±
include_batch_report_routes(app)

# æ·»åŠ åˆ†æ‰¹è¾“å‡ºç®¡ç†å™¨APIè·¯ç”±
app.include_router(batch_router)

# æ·»åŠ å¢å¼ºæŠ¥å‘Šç®¡ç†APIè·¯ç”±
try:
    from src.server.enhanced_report_api import router as report_router
    app.include_router(report_router)
    logger.info("âœ… å¢å¼ºæŠ¥å‘Šç®¡ç†APIå·²åŠ è½½")
except ImportError as e:
    logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å¢å¼ºæŠ¥å‘ŠAPI: {e}")

# æ·»åŠ å¥åº·æ£€æŸ¥APIè·¯ç”±
try:
    from src.server.health_api import router as health_router
    app.include_router(health_router)
    logger.info("âœ… å¥åº·æ£€æŸ¥APIå·²åŠ è½½")
except ImportError as e:
    logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å¥åº·æ£€æŸ¥API: {e}")

# æ·»åŠ Geminiæ·±åº¦ç ”ç©¶APIè·¯ç”±
try:
    from src.server.gemini_deep_research_api import router as gemini_research_router
    app.include_router(gemini_research_router)
    logger.info("âœ… Geminiæ·±åº¦ç ”ç©¶APIå·²åŠ è½½")
except ImportError as e:
    logger.warning(f"âš ï¸ æ— æ³•åŠ è½½Geminiæ·±åº¦ç ”ç©¶API: {e}")

# ==================== æœ¬åœ°è¯­éŸ³è¯†åˆ«é›†æˆ ====================

@app.get("/speech")
async def speech_interface():
    """
    è¿”å›æœ¬åœ°è¯­éŸ³è¯†åˆ«æµ‹è¯•é¡µé¢
    """
    # è¿”å›å†…è”HTMLé¡µé¢ï¼Œä¸ä¾èµ–å¤–éƒ¨æ–‡ä»¶
    html_content = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æœ¬åœ°è¯­éŸ³è¯†åˆ«</title>
    <style>
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: white;
        }
        .container {
            background: rgba(255,255,255,0.1);
            border-radius: 15px;
            padding: 30px;
            backdrop-filter: blur(10px);
        }
        .mic-button {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            background: linear-gradient(135deg, #4CAF50, #45a049);
            border: none;
            color: white;
            font-size: 40px;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 8px 25px rgba(0,0,0,0.3);
            margin: 20px auto;
            display: block;
        }
        .mic-button:hover {
            transform: scale(1.05);
        }
        .mic-button.recording {
            background: linear-gradient(135deg, #f44336, #d32f2f);
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
        .status {
            text-align: center;
            margin: 20px 0;
            font-size: 1.2em;
            font-weight: 500;
        }
        .result {
            background: rgba(255,255,255,0.9);
            color: #333;
            padding: 20px;
            border-radius: 10px;
            margin-top: 20px;
            min-height: 100px;
            font-size: 1.1em;
            line-height: 1.6;
        }
        .error {
            background: rgba(244, 67, 54, 0.9);
            color: white;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
        }
        .controls {
            text-align: center;
            margin: 20px 0;
        }
        .lang-select {
            padding: 10px;
            border-radius: 5px;
            border: none;
            margin: 0 10px;
            font-size: 1em;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 style="text-align: center;">ğŸ¤ æœ¬åœ°è¯­éŸ³è¯†åˆ«</h1>
        
        <div class="controls">
            <select id="languageSelect" class="lang-select">
                <option value="zh-CN">ä¸­æ–‡</option>
                <option value="en-US">English</option>
                <option value="ja-JP">æ—¥æœ¬èª</option>
            </select>
        </div>
        
        <button id="micButton" class="mic-button">ğŸ¤</button>
        <div id="status" class="status">ç‚¹å‡»éº¦å…‹é£å¼€å§‹è¯­éŸ³è¯†åˆ«</div>
        <div id="error" class="error" style="display: none;"></div>
        
        <div class="result">
            <h3>è¯†åˆ«ç»“æœï¼š</h3>
            <div id="result">ç­‰å¾…è¯­éŸ³è¾“å…¥...</div>
        </div>
    </div>

    <script>
        class LocalSpeechRecognition {
            constructor() {
                this.recognition = null;
                this.isRecording = false;
                this.initRecognition();
            }

                         initRecognition() {
                 console.log('ğŸ”§ åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«...');
                 
                 // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
                 const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                 if (!SpeechRecognition) {
                     console.error('âŒ æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«');
                     const errorDiv = document.getElementById('error');
                     errorDiv.style.display = 'block';
                     errorDiv.textContent = 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ã€‚è¯·ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„Chromeã€Edgeæˆ–Safariæµè§ˆå™¨ã€‚';
                     return false;
                 }
                 
                 this.recognition = new SpeechRecognition();
                 console.log('âœ… è¯­éŸ³è¯†åˆ«å¯¹è±¡åˆ›å»ºæˆåŠŸ');

                this.recognition.continuous = true;
                this.recognition.interimResults = true;
                this.recognition.lang = 'zh-CN';

                                 this.recognition.onstart = () => {
                     console.log('ğŸ¤ è¯­éŸ³è¯†åˆ«å·²å¯åŠ¨');
                     this.isRecording = true;
                     this.updateUI();
                     
                     const statusDiv = document.getElementById('status');
                     statusDiv.textContent = 'æ­£åœ¨å¬å–æ‚¨çš„è¯­éŸ³...';
                 };

                this.recognition.onresult = (event) => {
                    let finalTranscript = '';
                    let interimTranscript = '';

                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript;
                        } else {
                            interimTranscript += event.results[i][0].transcript;
                        }
                    }

                    const resultDiv = document.getElementById('result');
                    if (finalTranscript) {
                        resultDiv.innerHTML += '<div><strong>æœ€ç»ˆç»“æœï¼š</strong>' + finalTranscript + '</div>';
                        
                        // é€šè¿‡WebSocketå‘é€åˆ°æœåŠ¡å™¨
                        this.sendToServer(finalTranscript);
                    }
                    if (interimTranscript) {
                        resultDiv.innerHTML += '<div style="color: #666;"><em>ä¸´æ—¶ç»“æœï¼š</em>' + interimTranscript + '</div>';
                    }
                };

                this.recognition.onerror = (event) => {
                    console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
                    const errorDiv = document.getElementById('error');
                    errorDiv.style.display = 'block';
                    errorDiv.textContent = 'è¯­éŸ³è¯†åˆ«é”™è¯¯: ' + event.error;
                };

                this.recognition.onend = () => {
                    console.log('ğŸ›‘ è¯­éŸ³è¯†åˆ«å·²ç»“æŸ');
                    this.isRecording = false;
                    this.updateUI();
                };

                return true;
            }

            setLanguage(lang) {
                if (this.recognition) {
                    this.recognition.lang = lang;
                }
            }

                         start() {
                 if (this.recognition && !this.isRecording) {
                     console.log('ğŸš€ å¯åŠ¨è¯­éŸ³è¯†åˆ«...');
                     try {
                         this.recognition.start();
                         console.log('âœ… è¯­éŸ³è¯†åˆ«å¯åŠ¨æˆåŠŸ');
                     } catch (error) {
                         console.error('âŒ å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
                         const errorDiv = document.getElementById('error');
                         errorDiv.style.display = 'block';
                         errorDiv.textContent = 'å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥: ' + error.message;
                     }
                 } else {
                     console.log('âš ï¸ è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨æˆ–æ­£åœ¨å½•éŸ³ä¸­');
                 }
             }

            stop() {
                if (this.recognition && this.isRecording) {
                    this.recognition.stop();
                }
            }

            updateUI() {
                const button = document.getElementById('micButton');
                const status = document.getElementById('status');
                
                if (this.isRecording) {
                    button.classList.add('recording');
                    button.textContent = 'ğŸ›‘';
                    status.textContent = 'æ­£åœ¨å½•éŸ³... ç‚¹å‡»åœæ­¢';
                } else {
                    button.classList.remove('recording');
                    button.textContent = 'ğŸ¤';
                    status.textContent = 'ç‚¹å‡»éº¦å…‹é£å¼€å§‹è¯­éŸ³è¯†åˆ«';
                }
            }

            sendToServer(text) {
                // é€šè¿‡WebSocketå‘é€è¯†åˆ«ç»“æœåˆ°æœåŠ¡å™¨
                fetch('/api/speech/result', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        text: text,
                        timestamp: new Date().toISOString(),
                        language: this.recognition.lang
                    })
                }).then(response => response.json())
                  .then(data => console.log('æœåŠ¡å™¨å“åº”:', data))
                  .catch(error => console.error('å‘é€åˆ°æœåŠ¡å™¨å¤±è´¥:', error));
            }
        }

                 // æ£€æŸ¥éº¦å…‹é£æƒé™
         async function checkMicrophonePermission() {
             try {
                 const result = await navigator.permissions.query({ name: 'microphone' });
                 console.log('ğŸ¤ éº¦å…‹é£æƒé™çŠ¶æ€:', result.state);
                 
                 if (result.state === 'denied') {
                     const errorDiv = document.getElementById('error');
                     errorDiv.style.display = 'block';
                     errorDiv.textContent = 'éº¦å…‹é£æƒé™è¢«æ‹’ç»ã€‚è¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®æƒé™ã€‚';
                     return false;
                 }
                 return true;
             } catch (error) {
                 console.log('ğŸ“± æ— æ³•æ£€æŸ¥éº¦å…‹é£æƒé™ï¼Œå¯èƒ½æ˜¯æ—§ç‰ˆæµè§ˆå™¨');
                 return true; // æ—§ç‰ˆæµè§ˆå™¨å¯èƒ½ä¸æ”¯æŒæƒé™API
             }
         }

         // åˆå§‹åŒ–
         document.addEventListener('DOMContentLoaded', async () => {
             console.log('ğŸš€ é¡µé¢åŠ è½½å®Œæˆï¼Œå¼€å§‹åˆå§‹åŒ–...');
             
             // æ£€æŸ¥éº¦å…‹é£æƒé™
             const hasPermission = await checkMicrophonePermission();
             if (!hasPermission) {
                 document.getElementById('micButton').disabled = true;
                 return;
             }
             
             const speechRecognition = new LocalSpeechRecognition();
             
             if (!speechRecognition.recognition) {
                 document.getElementById('error').style.display = 'block';
                 document.getElementById('error').textContent = 'æŠ±æ­‰ï¼Œæ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚è¯·ä½¿ç”¨Chromeã€Edgeæˆ–Safariã€‚';
                 document.getElementById('micButton').disabled = true;
                 return;
             }

                         // éº¦å…‹é£æŒ‰é’®äº‹ä»¶
             document.getElementById('micButton').addEventListener('click', () => {
                 console.log('ğŸ–±ï¸ éº¦å…‹é£æŒ‰é’®è¢«ç‚¹å‡»');
                 console.log('ğŸ“Š å½“å‰å½•éŸ³çŠ¶æ€:', speechRecognition.isRecording);
                 
                 // æ¸…é™¤ä¹‹å‰çš„é”™è¯¯ä¿¡æ¯
                 document.getElementById('error').style.display = 'none';
                 
                 if (speechRecognition.isRecording) {
                     console.log('ğŸ›‘ åœæ­¢å½•éŸ³');
                     speechRecognition.stop();
                 } else {
                     console.log('ğŸ¤ å¼€å§‹å½•éŸ³');
                     speechRecognition.start();
                 }
             });

            // è¯­è¨€é€‰æ‹©äº‹ä»¶
            document.getElementById('languageSelect').addEventListener('change', (e) => {
                speechRecognition.setLanguage(e.target.value);
            });

            // æ¸…é™¤é”™è¯¯ä¿¡æ¯
            document.getElementById('micButton').addEventListener('click', () => {
                document.getElementById('error').style.display = 'none';
            });
        });
    </script>
</body>
</html>
    """
    return Response(content=html_content, media_type="text/html")

@app.post("/api/speech/result")
async def speech_result(request: Request):
    """
    æ¥æ”¶æµè§ˆå™¨è¯­éŸ³è¯†åˆ«ç»“æœ
    """
    try:
        data = await request.json()
        text = data.get("text", "")
        timestamp = data.get("timestamp", "")
        language = data.get("language", "zh-CN")
        
        logger.info(f"ğŸ¤ æ”¶åˆ°è¯­éŸ³è¯†åˆ«ç»“æœ: {text} (è¯­è¨€: {language})")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ åç»­å¤„ç†é€»è¾‘
        # æ¯”å¦‚ä¿å­˜åˆ°æ•°æ®åº“ã€è§¦å‘å…¶ä»–æœåŠ¡ç­‰
        
        return {
            "status": "success",
            "message": "è¯­éŸ³è¯†åˆ«ç»“æœå·²æ¥æ”¶",
            "data": {
                "text": text,
                "timestamp": timestamp,
                "language": language,
                "length": len(text)
            }
        }
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯­éŸ³è¯†åˆ«ç»“æœæ—¶å‡ºé”™: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }

@app.get("/api/speech/status")
async def speech_status():
    """
    æ£€æŸ¥æœ¬åœ°è¯­éŸ³è¯†åˆ«æœåŠ¡çŠ¶æ€
    """
    return {
        "status": "online",
        "type": "local_browser_speech_recognition",
        "message": "æœ¬åœ°è¯­éŸ³è¯†åˆ«æœåŠ¡æ­£å¸¸",
        "supported_languages": [
            {"code": "zh-CN", "name": "ä¸­æ–‡"},
            {"code": "en-US", "name": "English"},
            {"code": "ja-JP", "name": "æ—¥æœ¬èª"},
            {"code": "ko-KR", "name": "í•œêµ­ì–´"},
            {"code": "es-ES", "name": "EspaÃ±ol"},
            {"code": "fr-FR", "name": "FranÃ§ais"},
            {"code": "de-DE", "name": "Deutsch"}
        ],
        "note": "ä¾èµ–æµè§ˆå™¨å†…ç½®è¯­éŸ³è¯†åˆ«APIï¼Œéœ€è¦Chromeã€Edgeæˆ–Safariæµè§ˆå™¨"
    }

# ==================== åŸæœ‰çš„è·¯ç”±ä¿æŒä¸å˜ ====================

@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    thread_id = request.thread_id
    if thread_id == "__default__":
        thread_id = str(uuid4())
    return StreamingResponse(
        _astream_workflow_generator(
            request.model_dump()["messages"],
            thread_id,
            request.max_plan_iterations,
            request.max_step_num,
            request.max_search_results,
            request.auto_accepted_plan,
            request.interrupt_feedback,
            request.mcp_settings,
            request.enable_background_investigation,
            request.enable_multi_model_report,
        ),
        media_type="text/event-stream",
    )


async def _astream_workflow_generator(
    messages: List[ChatMessage],
    thread_id: str,
    max_plan_iterations: int,
    max_step_num: int,
    max_search_results: int,
    auto_accepted_plan: bool,
    interrupt_feedback: str,
    mcp_settings: dict,
    enable_background_investigation,
    enable_multi_model_report: bool,
):
    input_ = {
        "messages": messages,
        "plan_iterations": 0,
        "final_report": "",
        "current_plan": None,
        "observations": [],
        "auto_accepted_plan": auto_accepted_plan,
        "enable_background_investigation": enable_background_investigation,
        "enable_multi_model_report": enable_multi_model_report,
    }
    if not auto_accepted_plan and interrupt_feedback:
        if interrupt_feedback == "accepted":
            # ç®€å•çš„æ¥å—åé¦ˆï¼Œä¸å½±å“LangGraphæµå¤„ç†
            resume_msg = "[accepted] å®Œç¾ï¼å¼€å§‹ç ”ç©¶å§ã€‚"
        else:
            resume_msg = f"[{interrupt_feedback}]"
            # add the last message to the resume message  
            if messages:
                resume_msg += f" {messages[-1]['content']}"
        
        input_ = Command(resume=resume_msg)
    async for agent, _, event_data in graph.astream(
        input_,
        config={
            "thread_id": thread_id,
            "max_plan_iterations": max_plan_iterations,
            "max_step_num": max_step_num,
            "max_search_results": max_search_results,
            "mcp_settings": mcp_settings,
            "recursion_limit": 200,
        },
        stream_mode=["messages", "updates"],
        subgraphs=True,
    ):
        if isinstance(event_data, dict):
            if "__interrupt__" in event_data:
                yield _make_event("interrupt", {
                    "thread_id": thread_id,
                    "id": event_data["__interrupt__"][0].ns[0],
                    "role": "assistant",
                    "content": event_data["__interrupt__"][0].value,
                    "finish_reason": "interrupt",
                    "options": [
                        {"text": "Edit plan", "value": "edit_plan"},
                        {"text": "Start research", "value": "accepted"},
                    ],
                })
            continue
        message_chunk, message_metadata = cast(
            tuple[BaseMessage, dict[str, any]], event_data
        )
        event_stream_message: dict[str, any] = {
            "thread_id": thread_id,
            "agent": agent[0].split(":")[0],
            "id": message_chunk.id,
            "role": "assistant",
            "content": message_chunk.content,
        }
        if message_chunk.response_metadata.get("finish_reason"):
            event_stream_message["finish_reason"] = message_chunk.response_metadata.get(
                "finish_reason"
            )
        if isinstance(message_chunk, ToolMessage):
            # Tool Message - Return the result of the tool call
            event_stream_message["tool_call_id"] = message_chunk.tool_call_id
            yield _make_event("tool_call_result", event_stream_message)
        elif isinstance(message_chunk, AIMessageChunk):
            # AI Message - Raw message tokens
            if message_chunk.tool_calls:
                # AI Message - Tool Call
                event_stream_message["tool_calls"] = message_chunk.tool_calls
                event_stream_message["tool_call_chunks"] = (
                    message_chunk.tool_call_chunks
                )
                yield _make_event("tool_calls", event_stream_message)
            elif message_chunk.tool_call_chunks:
                # AI Message - Tool Call Chunks
                event_stream_message["tool_call_chunks"] = (
                    message_chunk.tool_call_chunks
                )
                yield _make_event("tool_call_chunks", event_stream_message)
            else:
                # AI Message - Raw message tokens
                yield _make_event("message_chunk", event_stream_message)


def _make_event(event_type: str, data: dict[str, any]):
    if data.get("content") == "":
        data.pop("content")
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


@app.post("/api/tts")
async def text_to_speech(request: TTSRequest):
    """Convert text to speech using volcengine TTS API."""
    try:
        app_id = os.getenv("VOLCENGINE_TTS_APPID", "")
        if not app_id:
            raise HTTPException(
                status_code=400, detail="VOLCENGINE_TTS_APPID is not set"
            )
        access_token = os.getenv("VOLCENGINE_TTS_ACCESS_TOKEN", "")
        if not access_token:
            raise HTTPException(
                status_code=400, detail="VOLCENGINE_TTS_ACCESS_TOKEN is not set"
            )
        cluster = os.getenv("VOLCENGINE_TTS_CLUSTER", "volcano_tts")
        voice_type = os.getenv("VOLCENGINE_TTS_VOICE_TYPE", "BV700_V2_streaming")

        tts_client = VolcengineTTS(
            appid=app_id,
            access_token=access_token,
            cluster=cluster,
            voice_type=voice_type,
        )
        # Call the TTS API
        result = tts_client.text_to_speech(
            text=request.text[:1024],
            encoding=request.encoding,
            speed_ratio=request.speed_ratio,
            volume_ratio=request.volume_ratio,
            pitch_ratio=request.pitch_ratio,
            text_type=request.text_type,
            with_frontend=request.with_frontend,
            frontend_type=request.frontend_type,
        )

        if not result["success"]:
            raise HTTPException(status_code=500, detail=str(result["error"]))

        # Decode the base64 audio data
        audio_data = base64.b64decode(result["audio_data"])

        # Return the audio file
        return Response(
            content=audio_data,
            media_type=f"audio/{request.encoding}",
            headers={
                "Content-Disposition": (
                    f"attachment; filename=tts_output.{request.encoding}"
                )
            },
        )
    except Exception as e:
        logger.exception(f"Error in TTS endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/podcast/generate")
async def generate_podcast(request: GeneratePodcastRequest):
    try:
        report_content = request.content
        print(report_content)
        workflow = build_podcast_graph()
        final_state = workflow.invoke({"input": report_content})
        audio_bytes = final_state["output"]
        return Response(content=audio_bytes, media_type="audio/mp3")
    except Exception as e:
        logger.exception(f"Error occurred during podcast generation: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/ppt/generate")
async def generate_ppt(request: GeneratePPTRequest):
    try:
        report_content = request.content
        print(report_content)
        workflow = build_ppt_graph()
        final_state = workflow.invoke({"input": report_content})
        generated_file_path = final_state["generated_file_path"]
        with open(generated_file_path, "rb") as f:
            ppt_bytes = f.read()
        return Response(
            content=ppt_bytes,
            media_type="application/vnd.openxmlformats-officedocument.presentationml.presentation",
        )
    except Exception as e:
        logger.exception(f"Error occurred during ppt generation: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/prose/generate")
async def generate_prose(request: GenerateProseRequest):
    try:
        logger.info(f"Generating prose for prompt: {request.prompt}")
        workflow = build_prose_graph()
        events = workflow.astream(
            {
                "content": request.prompt,
                "option": request.option,
                "command": request.command,
            },
            stream_mode="messages",
            subgraphs=True,
        )
        return StreamingResponse(
            (f"data: {event[0].content}\n\n" async for _, event in events),
            media_type="text/event-stream",
        )
    except Exception as e:
        logger.exception(f"Error occurred during prose generation: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/mcp/server/metadata", response_model=MCPServerMetadataResponse)
async def mcp_server_metadata(request: MCPServerMetadataRequest):
    """Get information about an MCP server."""
    try:
        # Set default timeout with a longer value for this endpoint
        timeout = 300  # Default to 300 seconds for this endpoint

        # Use custom timeout from request if provided
        if request.timeout_seconds is not None:
            timeout = request.timeout_seconds

        # Load tools from the MCP server using the utility function
        tools = await load_mcp_tools(
            server_type=request.transport,
            command=request.command,
            args=request.args,
            url=request.url,
            env=request.env,
            timeout_seconds=timeout,
        )

        # Create the response with tools
        response = MCPServerMetadataResponse(
            transport=request.transport,
            command=request.command,
            args=request.args,
            url=request.url,
            env=request.env,
            tools=tools,
        )

        return response
    except Exception as e:
        if not isinstance(e, HTTPException):
            logger.exception(f"Error in MCP server metadata endpoint: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))
        raise


@app.post("/api/report/section/save")
async def save_report_section(request: ReportSectionRequest):
    """
    ä¿å­˜æŠ¥å‘Šç« èŠ‚åˆ°æœ¬åœ°å­˜å‚¨
    
    @param {ReportSectionRequest} request - åŒ…å«ç« èŠ‚ä¿¡æ¯çš„è¯·æ±‚
    @returns {dict} ä¿å­˜ç»“æœå’Œç« èŠ‚è·¯å¾„
    """
    try:
        from src.utils.report_manager import ReportManager
        
        # åˆ›å»ºæŠ¥å‘Šç®¡ç†å™¨å®ä¾‹
        report_manager = ReportManager(
            report_name=request.report_name,
            base_dir=request.base_dir or "./outputs/reports",
            keep_chunks=request.keep_chunks
        )
        
        # ä¿å­˜ç« èŠ‚
        section_path = report_manager.save_section(
            title=request.section_title,
            content=request.section_content,
            section_number=request.section_number,
            metadata=request.section_metadata
        )
        
        return {
            "success": True,
            "section_path": section_path,
            "section_number": request.section_number,
            "section_title": request.section_title,
            "message": f"ç« èŠ‚ '{request.section_title}' å·²æˆåŠŸä¿å­˜"
        }
        
    except Exception as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šç« èŠ‚æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜ç« èŠ‚å¤±è´¥: {str(e)}")


@app.post("/api/report/merge")
async def merge_report_sections(request: ReportMergeRequest):
    """
    åˆå¹¶æ‰€æœ‰æŠ¥å‘Šç« èŠ‚ä¸ºå®Œæ•´æŠ¥å‘Š
    
    @param {ReportMergeRequest} request - åˆå¹¶è¯·æ±‚å‚æ•°
    @returns {dict} åˆå¹¶ç»“æœå’Œå®Œæ•´æŠ¥å‘Šè·¯å¾„
    """
    try:
        from src.utils.report_manager import ReportManager
        
        # åˆ›å»ºæŠ¥å‘Šç®¡ç†å™¨å®ä¾‹
        report_manager = ReportManager(
            report_name=request.report_name,
            base_dir=request.base_dir or "./outputs/reports",
            keep_chunks=request.keep_chunks
        )
        
        # åˆå¹¶æŠ¥å‘Š
        final_path = report_manager.merge_report(
            include_toc=request.include_toc,
            sort_by_number=request.sort_by_number
        )
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = report_manager.get_stats()
        
        return {
            "success": True,
            "final_report_path": final_path,
            "report_stats": stats,
            "message": f"æŠ¥å‘Šå·²æˆåŠŸåˆå¹¶ï¼Œå…± {stats['total_sections']} ä¸ªç« èŠ‚"
        }
        
    except Exception as e:
        logger.error(f"åˆå¹¶æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆå¹¶æŠ¥å‘Šå¤±è´¥: {str(e)}")


@app.get("/api/report/download/{report_name}")
async def download_report(report_name: str, base_dir: str = "./outputs/reports"):
    """
    ä¸‹è½½å®Œæ•´æŠ¥å‘Šæ–‡ä»¶
    
    @param {string} report_name - æŠ¥å‘Šåç§°
    @param {string} base_dir - æŠ¥å‘Šå­˜å‚¨ç›®å½•
    @returns {Response} æ–‡ä»¶ä¸‹è½½å“åº”
    """
    try:
        import os
        from fastapi.responses import FileResponse
        
        # æ„å»ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        report_path = os.path.join(base_dir, report_name, f"{report_name}_complete.md")
        
        if not os.path.exists(report_path):
            raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")
        
        return FileResponse(
            path=report_path,
            filename=f"{report_name}_complete.md",
            media_type="text/markdown"
        )
        
    except Exception as e:
        logger.error(f"ä¸‹è½½æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æŠ¥å‘Šå¤±è´¥: {str(e)}")


@app.get("/api/report/list")
async def list_reports(base_dir: str = "./outputs/reports"):
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æŠ¥å‘Š
    
    @param {string} base_dir - æŠ¥å‘Šå­˜å‚¨ç›®å½•
    @returns {dict} æŠ¥å‘Šåˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    try:
        import os
        from datetime import datetime
        
        reports = []
        
        if os.path.exists(base_dir):
            for item in os.listdir(base_dir):
                item_path = os.path.join(base_dir, item)
                if os.path.isdir(item_path):
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´æŠ¥å‘Šæ–‡ä»¶
                    complete_file = os.path.join(item_path, f"{item}_complete.md")
                    if os.path.exists(complete_file):
                        stat = os.stat(complete_file)
                        reports.append({
                            "name": item,
                            "path": complete_file,
                            "size": stat.st_size,
                            "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                            "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat()
                        })
        
        return {
            "success": True,
            "reports": reports,
            "total_count": len(reports)
        }
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æŠ¥å‘Šåˆ—è¡¨å¤±è´¥: {str(e)}")


@app.post("/api/chat/stream/large")
async def chat_stream_large_report(request: LargeReportRequest):
    """
    å¤„ç†å¤§å‹æŠ¥å‘Šç”Ÿæˆçš„æµå¼å“åº”ï¼Œæ”¯æŒåˆ†æ‰¹è¾“å‡º
    
    @param {LargeReportRequest} request - å¤§å‹æŠ¥å‘Šè¯·æ±‚
    @returns {StreamingResponse} åˆ†æ‰¹æµå¼å“åº”
    """
    thread_id = request.thread_id
    if thread_id == "__default__":
        thread_id = str(uuid4())
    
    return StreamingResponse(
        _astream_large_report_generator(
            request.model_dump()["messages"],
            thread_id,
            request.max_plan_iterations,
            request.max_step_num,
            request.max_search_results,
            request.auto_accepted_plan,
            request.interrupt_feedback,
            request.mcp_settings,
            request.enable_background_investigation,
            request.enable_multi_model_report,
            request.report_name,
            request.chunk_size,
            request.auto_save_sections,
            request.base_dir,
        ),
        media_type="text/event-stream",
    )


async def _astream_large_report_generator(
    messages: List[ChatMessage],
    thread_id: str,
    max_plan_iterations: int,
    max_step_num: int,
    max_search_results: int,
    auto_accepted_plan: bool,
    interrupt_feedback: str,
    mcp_settings: dict,
    enable_background_investigation: bool,
    enable_multi_model_report: bool,
    report_name: str,
    chunk_size: int,
    auto_save_sections: bool,
    base_dir: str,
):
    """
    å¤§å‹æŠ¥å‘Šçš„æµå¼ç”Ÿæˆå™¨ï¼Œæ”¯æŒåˆ†æ‰¹è¾“å‡ºå’Œè‡ªåŠ¨ä¿å­˜
    """
    from src.utils.report_manager import ReportManager
    
    # åˆå§‹åŒ–æŠ¥å‘Šç®¡ç†å™¨
    report_manager = None
    if auto_save_sections:
        report_manager = ReportManager(
            report_name=report_name,
            base_dir=base_dir or "./outputs/reports",
            keep_chunks=True
        )
    
    current_section = ""
    section_count = 0
    accumulated_content = ""
    
    input_ = {
        "messages": messages,
        "plan_iterations": 0,
        "final_report": "",
        "current_plan": None,
        "observations": [],
        "auto_accepted_plan": auto_accepted_plan,
        "enable_background_investigation": enable_background_investigation,
        "enable_multi_model_report": enable_multi_model_report,
    }
    
    if not auto_accepted_plan and interrupt_feedback:
        if interrupt_feedback == "accepted":
            # ç®€å•çš„æ¥å—åé¦ˆï¼Œä¸å½±å“LangGraphæµå¤„ç†
            resume_msg = "[accepted] å®Œç¾ï¼å¼€å§‹ç ”ç©¶å§ã€‚"
        else:
            resume_msg = f"[{interrupt_feedback}]"
            # add the last message to the resume message  
            if messages:
                resume_msg += f" {messages[-1]['content']}"
        
        input_ = Command(resume=resume_msg)
    
    async for agent, _, event_data in graph.astream(
        input_,
        config={
            "thread_id": thread_id,
            "max_plan_iterations": max_plan_iterations,
            "max_step_num": max_step_num,
            "max_search_results": max_search_results,
            "mcp_settings": mcp_settings,
            "recursion_limit": 200,
        },
        stream_mode=["messages", "updates"],
        subgraphs=True,
    ):
        if isinstance(event_data, dict):
            if "__interrupt__" in event_data:
                yield _make_event(
                    "interrupt",
                    {
                        "thread_id": thread_id,
                        "id": event_data["__interrupt__"][0].ns[0],
                        "role": "assistant",
                        "content": event_data["__interrupt__"][0].value,
                        "finish_reason": "interrupt",
                        "options": [
                            {"text": "Edit plan", "value": "edit_plan"},
                            {"text": "Start research", "value": "accepted"},
                        ],
                    },
                )
            continue
            
        message_chunk, message_metadata = cast(
            tuple[BaseMessage, dict[str, any]], event_data
        )
        
        content = message_chunk.content
        accumulated_content += content
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ‰¹è¾“å‡º
        if len(accumulated_content) >= chunk_size:
            # å°è¯•åœ¨åˆé€‚çš„ä½ç½®åˆ†å‰²ï¼ˆå¦‚æ®µè½ç»“æŸï¼‰
            split_pos = accumulated_content.rfind('\n\n')
            if split_pos == -1:
                split_pos = accumulated_content.rfind('\n')
            if split_pos == -1:
                split_pos = chunk_size
            
            chunk_content = accumulated_content[:split_pos]
            accumulated_content = accumulated_content[split_pos:]
            
            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
            if chunk_content.strip().startswith('#'):
                section_count += 1
                section_title = chunk_content.split('\n')[0].strip('#').strip()
                
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section and report_manager:
                    try:
                        section_path = report_manager.save_section(
                            title=f"Section_{section_count-1}",
                            content=current_section,
                            section_number=section_count-1
                        )
                        yield _make_event("section_saved", {
                            "thread_id": thread_id,
                            "section_number": section_count-1,
                            "section_path": section_path,
                            "message": f"ç« èŠ‚ {section_count-1} å·²ä¿å­˜"
                        })
                    except Exception as e:
                        logger.error(f"ä¿å­˜ç« èŠ‚æ—¶å‡ºé”™: {str(e)}")
                
                current_section = chunk_content
            else:
                current_section += chunk_content
            
            # å‘é€åˆ†å—å†…å®¹
            event_stream_message = {
                "thread_id": thread_id,
                "agent": agent[0].split(":")[0],
                "id": message_chunk.id,
                "role": "assistant",
                "content": chunk_content,
                "chunk_number": section_count,
                "is_chunk": True,
            }
            
            if message_chunk.response_metadata.get("finish_reason"):
                event_stream_message["finish_reason"] = message_chunk.response_metadata.get("finish_reason")
            
            yield _make_event("message_chunk", event_stream_message)
        
        # å¤„ç†å…¶ä»–ç±»å‹çš„æ¶ˆæ¯
        elif isinstance(message_chunk, ToolMessage):
            event_stream_message = {
                "thread_id": thread_id,
                "agent": agent[0].split(":")[0],
                "id": message_chunk.id,
                "role": "assistant",
                "content": content,
                "tool_call_id": message_chunk.tool_call_id,
            }
            yield _make_event("tool_call_result", event_stream_message)
        
        elif isinstance(message_chunk, AIMessageChunk):
            event_stream_message = {
                "thread_id": thread_id,
                "agent": agent[0].split(":")[0],
                "id": message_chunk.id,
                "role": "assistant",
                "content": content,
            }
            
            if message_chunk.response_metadata.get("finish_reason"):
                event_stream_message["finish_reason"] = message_chunk.response_metadata.get("finish_reason")
            
            if message_chunk.tool_calls:
                event_stream_message["tool_calls"] = message_chunk.tool_calls
                event_stream_message["tool_call_chunks"] = message_chunk.tool_call_chunks
                yield _make_event("tool_calls", event_stream_message)
            elif message_chunk.tool_call_chunks:
                event_stream_message["tool_call_chunks"] = message_chunk.tool_call_chunks
                yield _make_event("tool_call_chunks", event_stream_message)
            else:
                yield _make_event("message_chunk", event_stream_message)
    
    # å¤„ç†å‰©ä½™å†…å®¹å’Œæœ€åä¸€ä¸ªç« èŠ‚
    if accumulated_content.strip():
        current_section += accumulated_content
    
    if current_section and report_manager:
        try:
            section_path = report_manager.save_section(
                title=f"Section_{section_count}",
                content=current_section,
                section_number=section_count
            )
            yield _make_event("section_saved", {
                "thread_id": thread_id,
                "section_number": section_count,
                "section_path": section_path,
                "message": f"æœ€åç« èŠ‚ {section_count} å·²ä¿å­˜"
            })
            
            # è‡ªåŠ¨åˆå¹¶æŠ¥å‘Š
            final_path = report_manager.merge_report(include_toc=True, sort_by_number=True)
            stats = report_manager.get_stats()
            
            yield _make_event("report_completed", {
                "thread_id": thread_id,
                "final_report_path": final_path,
                "report_stats": stats,
                "message": f"å®Œæ•´æŠ¥å‘Šå·²ç”Ÿæˆï¼Œå…± {stats['total_sections']} ä¸ªç« èŠ‚"
            })
            
        except Exception as e:
            logger.error(f"å®ŒæˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")


@app.post("/api/generate-complete-report")
async def generate_complete_report_endpoint(request: Request):
    """
    ç”Ÿæˆå®Œæ•´çš„8æ‰¹æ¬¡ç ”ç©¶æŠ¥å‘Š
    
    @param {dict} request - åŒ…å«æŠ¥å‘Šåç§°å’Œå…¶ä»–é…ç½®çš„è¯·æ±‚
    @returns {dict} ç”Ÿæˆç»“æœå’Œè¿›åº¦ä¿¡æ¯
    """
    try:
        # è§£æè¯·æ±‚æ•°æ®
        data = await request.json()
        report_name = data.get("report_name", "DXAå½±åƒAIé¢„æµ‹ç ”ç©¶")
        
        # å¯¼å…¥å®Œæ•´æŠ¥å‘Šç”Ÿæˆå™¨
        from src.utils.complete_report_generator import CompleteReportGenerator
        from src.models.factory import get_model_client
        
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = CompleteReportGenerator(
            report_name=report_name,
            output_dir="./outputs/complete_reports"
        )
        
        # è·å–æ¨¡å‹å®¢æˆ·ç«¯
        model_client = get_model_client("gemini")
        
        # ç”ŸæˆæŠ¥å‘Š
        batch_results = []
        
        for batch_num in range(8):  # 8ä¸ªæ‰¹æ¬¡
            try:
                # è·å–å½“å‰æ‰¹æ¬¡çš„æç¤º
                prompt = generator.get_next_batch_prompt()
                if not prompt:
                    break
                
                logger.info(f"æ­£åœ¨ç”Ÿæˆç¬¬{batch_num + 1}/8æ‰¹æ¬¡...")
                
                # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå†…å®¹
                response = model_client.chat([{"role": "user", "content": prompt}])
                content = response.strip()
                
                # ä¿å­˜å½“å‰æ‰¹æ¬¡å†…å®¹
                batch_result = generator.save_section(content)
                batch_results.append(batch_result)
                
                logger.info(f"âœ… ç¬¬{batch_num + 1}æ‰¹æ¬¡å®Œæˆ: {batch_result['section_title']}")
                
                # æ·»åŠ å»¶æ—¶é¿å…APIé™åˆ¶
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"ç¬¬{batch_num + 1}æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥: {str(e)}")
                continue
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰æ‰¹æ¬¡
        if generator.is_complete():
            # ç”Ÿæˆæœ€ç»ˆå®Œæ•´æŠ¥å‘Š
            final_result = generator.generate_final_report()
            
            return {
                "success": True,
                "message": "å®Œæ•´æŠ¥å‘Šç”ŸæˆæˆåŠŸ",
                "final_result": final_result,
                "batch_results": batch_results,
                "progress": 100
            }
        else:
            # éƒ¨åˆ†å®Œæˆ
            progress = generator.get_progress()
            return {
                "success": False,
                "message": "æŠ¥å‘Šç”Ÿæˆéƒ¨åˆ†å®Œæˆ",
                "progress": progress['progress_percentage'],
                "batch_results": batch_results,
                "next_section": progress.get('next_section')
            }
            
    except Exception as e:
        logger.error(f"ç”Ÿæˆå®Œæ•´æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå¤±è´¥: {str(e)}")
